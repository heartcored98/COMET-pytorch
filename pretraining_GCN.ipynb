{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from dataloader import *\n",
    "\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()   \n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "            \n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "            \n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2)) \n",
    "        attn_matrix = torch.mul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):    \n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gconv, Readout, BN1D, ResBlock, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, attn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.attn = attn\n",
    "        if self.attn is None:\n",
    "            self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        if self.attn is None:\n",
    "            x = self.fc(X)\n",
    "            x = torch.matmul(A, x)\n",
    "        else:\n",
    "            x = self.attn(X, A)            \n",
    "        return x, A\n",
    "    \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = self.relu(torch.sum(molvec, dim=1))\n",
    "        return molvec\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x): \n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn, use_attn, dp_rate, sc_type, n_attn_head=None):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.sc_type = sc_type\n",
    "        \n",
    "        attn = Attention(in_dim, out_dim, n_attn_head) if use_attn else None\n",
    "        self.gconv = GConv(in_dim, out_dim, attn)\n",
    "        \n",
    "        self.bn1 = BN1d(out_dim, use_bn)\n",
    "        self.dropout = nn.Dropout2d(p=dp_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if not self.sc_type in ['no', 'gsc', 'sc']:\n",
    "            raise Exception\n",
    "\n",
    "        if self.sc_type != 'no':\n",
    "            self.bn2 = BN1d(out_dim, use_bn)\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if in_dim != out_dim:\n",
    "                self.shortcut.add_module('shortcut', nn.Linear(in_dim, out_dim, bias=False))\n",
    "                \n",
    "        if self.sc_type == 'gsc':\n",
    "            self.g_fc1 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.g_fc2 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, A):     \n",
    "        x, A = self.gconv(X, A)\n",
    "\n",
    "        if self.sc_type == 'no': #no skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            return self.dropout(x), A\n",
    "        \n",
    "        elif self.sc_type == 'sc': # basic skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            x = x + self.shortcut(X)          \n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "        elif self.sc_type == 'gsc': # gated skip-connection\n",
    "            x = self.relu(self.bn1(x)) \n",
    "            x1 = self.g_fc1(self.shortcut(X))\n",
    "            x2 = self.g_fc2(x)\n",
    "            gate_coef = self.sigmoid(x1+x2)\n",
    "            x = torch.mul(x1, gate_coef) + torch.mul(x2, 1.0-gate_coef)\n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "    \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bs = args.batch_size\n",
    "        self.molvec_dim = args.molvec_dim\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train) \n",
    "        self.out_dim = args.out_dim\n",
    "        \n",
    "        list_gconvs = nn.ModuleList()\n",
    "        for i in range(args.num_layers):\n",
    "            if i==0:\n",
    "                list_gconvs.append(ResBlock(args.in_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "            else:\n",
    "                list_gconvs.append(ResBlock(self.out_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "                \n",
    "        self.gconvs = list_gconvs\n",
    "        \n",
    "        self.readout = Readout(self.out_dim, self.molvec_dim)\n",
    "    \n",
    "    def forward(self, input_X, A):   \n",
    "        x, A, molvec = self.encoder(input_X, A)\n",
    "        return x, A, molvec\n",
    "     \n",
    "    def encoder(self, input_X, A):\n",
    "        x = self._embed(input_X)\n",
    "        for i, module in enumerate(self.gconvs):\n",
    "            x, A = module(x, A)\n",
    "        molvec = self.readout(x)\n",
    "        return x, A, molvec\n",
    "    \n",
    "    def _embed(self, x):\n",
    "        embed_x = self.embedding(x[:,:,0])\n",
    "        x = torch.cat((embed_x.float(), x[:,:,1:].float()), 2)\n",
    "        return x \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train=False):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred_x, ground_x, vocab_size):\n",
    "    batch_size = ground_x.shape[0]\n",
    "    num_masking = ground_x.shape[1]\n",
    "    ground_x = ground_x.view(batch_size * num_masking, -1)\n",
    "    \n",
    "    symbol_loss = F.cross_entropy(pred_x[:,:vocab_size], ground_x[:, 0].detach())\n",
    "    degree_loss = F.cross_entropy(pred_x[:,vocab_size:vocab_size+6], ground_x[:,1:7].detach().max(dim=1)[1])\n",
    "    numH_loss = F.cross_entropy(pred_x[:,vocab_size+6:vocab_size+11], ground_x[:, 7:12].detach().max(dim=1)[1])\n",
    "    valence_loss = F.cross_entropy(pred_x[:,vocab_size+11:vocab_size+17], ground_x[:,12:18].detach().max(dim=1)[1])\n",
    "    isarom_loss = F.binary_cross_entropy(torch.sigmoid(pred_x[:,-1]), ground_x[:,-1].detach().float())\n",
    "    total_loss = symbol_loss + degree_loss + numH_loss + valence_loss + isarom_loss\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier & Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, molvec_dim, vocab_size, dropout_rate=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.vs = vocab_size\n",
    "    \n",
    "        self.fc1 = nn.Linear(self.molvec_dim + self.out_dim, args.in_dim)\n",
    "        self.fc2 = nn.Linear(self.in_dim, args.in_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.param_initializer()\n",
    "        \n",
    "    def forward(self, X, molvec, idx_M):\n",
    "        batch_size = X.shape[0]\n",
    "        #print('idx_M', idx_M.shape)\n",
    "        num_masking = idx_M.shape[1]\n",
    "        probs_atom = list()\n",
    "        probs_degree = list()\n",
    "        probs_numH = list()\n",
    "        probs_valence = list()\n",
    "        probs_isarom = list()\n",
    "        \n",
    "        molvec = torch.unsqueeze(molvec, 1)\n",
    "        molvec = molvec.expand(batch_size, num_masking, molvec.shape[-1])\n",
    "        \n",
    "        list_concat_x = list()\n",
    "        for i in range(batch_size):\n",
    "            target_x = torch.index_select(X[i], 0, idx_M[i])\n",
    "            concat_x = torch.cat((target_x, molvec[i]), dim=1)\n",
    "            list_concat_x.append(concat_x)\n",
    "            \n",
    "        concat_x = torch.stack(list_concat_x)\n",
    "        pred_x = self.classify(concat_x)\n",
    "        pred_x = pred_x.view(batch_size * num_masking, -1)\n",
    "        return pred_x\n",
    "    \n",
    "    def classify(self, concat_x):\n",
    "        x = self.relu(self.fc1(concat_x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "    \n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, molvec_dim, dropout_rate):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.reg_fc1 = nn.Linear(self.molvec_dim, self.molvec_dim//2)\n",
    "        self.reg_fc2 = nn.Linear(self.molvec_dim//2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, molvec):\n",
    "        x = self.relu(self.reg_fc1(molvec))\n",
    "        x = self.reg_fc2(x)\n",
    "        return torch.squeeze(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, data_loader, optimizer, args, **kwargs):\n",
    "    t = time.time()\n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "\n",
    "    for _, model in models.items():\n",
    "        model.train()\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "        input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "        A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "        mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "        logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "        ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "        idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "        \n",
    "        \n",
    "        # Encoding Molecule\n",
    "        X, A, molvec = models['encoder'](input_X, A)\n",
    "        pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "        \n",
    "        \n",
    "        # Compute Mask Task Loss & Property Regression Loss\n",
    "        train_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "                        \n",
    "        if args.train_logp:\n",
    "            pred_logp = models['logP'](molvec)\n",
    "            train_loss += reg_loss(pred_logp, logp)\n",
    "        if args.train_mr:\n",
    "            pred_mr = models['mr'](molvec)\n",
    "            train_loss += reg_loss(pred_mr, mr)\n",
    "        if args.train_tpsa:\n",
    "            pred_tpsa = models['tpsa'](molvec)\n",
    "            train_loss += reg_loss(pred_tpsa, tpsa)\n",
    "        \n",
    "        #print(train_loss)\n",
    "        epoch_train_loss += train_loss\n",
    "        list_train_loss.append({'epoch':batch_idx/len(data_loader)+kwargs['epoch'], 'train_loss':train_loss})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt_iter += 1   \n",
    "        \n",
    "        \n",
    "        if cnt_iter % args.save_every:\n",
    "            pass\n",
    "        \n",
    "        if cnt_iter > 10:\n",
    "            break\n",
    "        print(time.time()-t, train_loss)\n",
    "        t = time.time()\n",
    "        \n",
    "    return models, list_train_loss\n",
    "\n",
    "\n",
    "def validate(models, data_loader, args, **kwargs):\n",
    "\n",
    "    t = time.time()\n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "    \n",
    "    list_logp, list_pred_logp = [], []\n",
    "    list_mr, list_pred_mr = [], []\n",
    "    list_tpsa, list_pred_tpsa = [], []\n",
    "    logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Initialization Model with Evaluation Mode\n",
    "    for _, model in models.items():\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "            input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "            A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "            mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "            logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "            ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "            idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "\n",
    "            # Encoding Molecule\n",
    "            X, A, molvec = models['encoder'](input_X, A)\n",
    "            pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "\n",
    "            # Compute Mask Task Loss & Property Regression Loss\n",
    "            val_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "\n",
    "            if args.train_logp:\n",
    "                pred_logp = models['logP'](molvec)\n",
    "                val_loss += reg_loss(pred_logp, logp)\n",
    "                list_logp += logp.cpu().detach().numpy().tolist()\n",
    "                list_pred_logp += pred_logp.cpu().detach().numpy().tolist()\n",
    "            \n",
    "            if args.train_mr:\n",
    "                pred_mr = models['mr'](molvec)\n",
    "                val_loss += reg_loss(pred_mr, mr)\n",
    "                list_mr += mr.cpu().detach().numpy().tolist()\n",
    "                list_pred_mr += pred_mr.cpu().detach().numpy().tolist()\n",
    "            \n",
    "            if args.train_tpsa:\n",
    "                pred_tpsa = models['tpsa'](molvec)\n",
    "                val_loss += reg_loss(pred_tpsa, tpsa)\n",
    "                list_tpsa += tpsa.cpu().detach().numpy().tolist()\n",
    "                list_pred_tpsa += pred_tpsa.cpu().detach().numpy().tolist()\n",
    "\n",
    "            #print(val_loss)\n",
    "            epoch_val_loss += val_loss\n",
    "            cnt_iter += 1   \n",
    "            if cnt_iter > 30:\n",
    "                break\n",
    "\n",
    "            if cnt_iter % args.save_every:\n",
    "                pass\n",
    "            print(time.time()-t, val_loss)\n",
    "            t = time.time()\n",
    "    \n",
    "    # Calculate overall MAE and STD value      \n",
    "    if args.train_logp:\n",
    "        logp_mae = mean_absolute_error(list_logp, list_pred_logp)\n",
    "        logp_std = np.std(np.array(list_logp)-np.array(list_pred_logp))\n",
    "        \n",
    "    if args.train_mr:\n",
    "        mr_mae = mean_absolute_error(list_mr, list_pred_mr)\n",
    "        mr_std = np.std(np.array(list_mr)-np.array(list_pred_mr))\n",
    "        \n",
    "    if args.train_tpsa:\n",
    "        tpsa_mae = mean_absolute_error(list_tpsa, list_pred_tpsa)\n",
    "        tpsa_std = np.std(np.array(list_tpsa)-np.array(list_pred_tpsa))\n",
    "                \n",
    "    torch.cuda.empty_cache()\n",
    "    print()\n",
    "    print(epoch_val_loss, logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std)\n",
    "    return epoch_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, model_name, epoch):\n",
    "    filename= '{}_{}_ckpt.pth'.format(model_name, epoch)\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "\n",
    "def experiment(data_loader, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    # Construct Model\n",
    "    encoder = Encoder(args)\n",
    "    classifier = Classifier(args.in_dim, args.out_dim, args.molvec_dim, args.vocab_size, args.dp_rate)\n",
    "    models = {'encoder': encoder, 'classifier': classifier}\n",
    "    if args.train_logp:\n",
    "        models.update({'logP': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_mr:\n",
    "        models.update({'mr': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_tpsa:\n",
    "        models.update({'tpsa': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = list()\n",
    "    for key, model in models.items():\n",
    "        model.to(args.device)\n",
    "        trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        print(key, sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_logp_mae = list()\n",
    "    list_logp_std = list()\n",
    "    list_mr_mae = list()\n",
    "    list_mr_std = list()\n",
    "    list_tpsa_mae = list()\n",
    "    list_tpsa_std = list()\n",
    "    \n",
    "    tot_iter = 0\n",
    "    \n",
    "    args.best_mae = 10000\n",
    "    for epoch in range(args.epoch):\n",
    "        \n",
    "        models, train_losses = train(models, data_loader, optimizer, args, **{'epoch':epoch})\n",
    "        print(\"start validation\")\n",
    "        val_losses = validate(models, data_loader, args, **{'epoch':epoch})\n",
    "        \n",
    "#         print(train_losses, val_losses)\n",
    "        \n",
    "\n",
    "\n",
    "        list_train_loss += train_losses\n",
    "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "        list_logp_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "        list_logp_std.append({'epoch':epoch, 'std':logp_std})\n",
    "        list_mr_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "        list_mr_std.append({'epoch':epoch, 'std':logp_std})\n",
    "        list_tpsa_mae.append({'epoch':epoch, 'mae':tpsa_mae})\n",
    "        list_tpsa_std.append({'epoch':epoch, 'std':tpsa_std})\n",
    "        \n",
    "        if args.best_mae > mae or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            #args.best_true_y = true_y\n",
    "            #args.best_pred_y = pred_y\n",
    "            \n",
    "        if total_iter % args.save_every == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': models['encoder'],\n",
    "                'encoder_state_dict': models['encoder'].state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                 })\n",
    "            \n",
    "        total_iter += 1\n",
    "             \n",
    "          \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.logp_maes = list_logp_mae\n",
    "    args.logp_stds = list_logp_std\n",
    "    args.tpsa_maes = list_tpsa_mae\n",
    "    args.tpsa_stds = list_tpsa_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp0'\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "\n",
    "##### SIZE #####\n",
    "args.vocab_size = 41\n",
    "args.in_dim = 59\n",
    "args.out_dim = 512\n",
    "args.molvec_dim = 512\n",
    "\n",
    "\n",
    "##### MODEL #####\n",
    "args.num_layers = 8\n",
    "args.use_attn = True\n",
    "args.n_attn_heads = 8\n",
    "args.use_bn = True\n",
    "args.sc_type = 'sc'\n",
    "args.emb_train = True\n",
    "args.train_logp = True\n",
    "args.train_mr = True\n",
    "args.train_tpsa = True\n",
    "\n",
    "##### HYPERPARAMETERS #####\n",
    "args.optim = 'ADAM'\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.dp_rate = 0.1\n",
    "\n",
    "##### EXP #####\n",
    "args.epoch = 2\n",
    "args.batch_size = 16\n",
    "args.test_batch_size = 16\n",
    "args.save_every = 50\n",
    "\n",
    "##### DEVICE #####\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/processed_zinc_smiles/data_xs/train'\n",
    "val_dataset_path = './dataset/processed_zinc_smiles/data_xs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)\n",
    "list_vals = get_dir_files(val_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = zincDataLoader(join(train_dataset_path, list_trains[0]),\n",
    "                                  batch_size=16,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=True,\n",
    "                                  num_workers=8)\n",
    "\n",
    "val_dataloader = zincDataLoader(join(val_dataset_path, list_vals[0]),\n",
    "                                  batch_size=16,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=True,\n",
    "                                  num_workers=8)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7c5a7ef28>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x7fa7fddefef0>]\n",
      " [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7fde00e48>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x7fa7c4e4a1d0>]\n",
      " [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7c4efc748>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x7fa7c4edacc0>]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X+UXWV97/H3pwiKQSQxdBoTZLCm1kiuBWaRdNFV54qGALZhtRahCMEbya1CxdX0alBvofgr3ntFiT9oo6QkXgpSRIkVjBGZa102KQkiESJN1CCThqSQQBiwxdHv/WM/QzYn58zsmTk/9jnn81rrrNnn2c/ez7PPefZ8z9772c9WRGBmZlbEr7W6AmZm1j4cNMzMrDAHDTMzK8xBw8zMCnPQMDOzwhw0zMysMAeNNiTpAUn9E1juBkkfbkCVzBpC0k5Jb2xymb2SQtILmlluu3DQaEMR8dqIGGh1Pcw6QSsCUztz0DAzs8IcNNrQyC8jSVdJukXSWklPpdNWfbl8J0m6N837EvCiivW8WdJ9kp6Q9D1J/yWl/6akfZJOTu9fLunfJ3JKzKweJP2apOWSfizp8dTup6V5I6eTFkv6maTHJH0gt+yRktZI2i9pm6T3ShpM874IvAL4mqQhSe/NFXtBtfV1OweN9veHwM3AMcA64DMAko4Avgp8EZgG/APwxyMLSToJWA38d+BlwN8C6yS9MCJ+DLwP+L+SXgz8HbDGp8Sshf4cOAd4PfByYD/w2Yo8vwe8Gjgd+CtJr0npVwK9wCuBNwFvG1kgIi4Efgb8QUQcFRH/q8D6upqDRvv7bkTcERG/JAsQr0vp84HDgU9FxC8i4lbgntxyS4G/jYhNEfHLiFgD/Gdajoj4PLAD2ATMAPxLy1rpz4APRMRgRPwncBXwloqL1X8dET+PiB8AP+DgvnAu8NGI2B8Rg8DKgmXWWl9Xc++A9vdobvoZ4EVpR3o5sCuePyLlw7np44HFkv48l3ZEWm7E58mOXpamHdWsVY4HviLpV7m0XwI9ufeV+8JRafrlwCO5efnp0dRaX1fzkUbn2g3MlKRc2ity048AH4mIY3KvF0fETQCSjgI+BVwPXDVy/tisRR4Bzqxory+KiF0Flt0NzMq9P65ivof6HgcHjc71z8Aw8G5Jh0v6I+DU3PzPA38maZ4yUySdLeklaf61wOaIeAfwdeBvmlp7s+f7G+Ajko4HkHSspEUFl70FuELSVEkzgcsq5u8hu95hBThodKiIeBb4I+BiYB/wVuC23PzNwCVkF873k12/uBgg7YwLgXem7H8BnCzpgubU3uwQ15KdKv2mpKeAjcC8gsteDQwCPwW+BdxKdv1uxMeAD6ZehH9Zvyp3JvkhTGbWTSS9EzgvIl7f6rq0Ix9pmFlHkzRD0mnpXo9XA8uAr7S6Xu3KvafMrNMdQXYf0gnAE2T3NX2upTVqYz49ZWZmhfn0lJmZFdZxp6emT58evb29h6Q//fTTTJkypfkVKoFu3faJbveWLVsei4hjG1ClhqjV5lupXdpcu9QTGl/Xou2+44JGb28vmzdvPiR9YGCA/v7+5leoBLp12ye63ZIeHjtXedRq863ULm2uXeoJja9r0Xbv01Nmhzpc0t2SHkwjB18OIGmapA2Stqe/U1O6JK2UtEPS/SOjA6d5i1P+7ZIW59JPkbQ1LbNy5M79WmWYlcWkjzQkHQZsJhvn6M2STiDrnfAyYAtwYUQ8K+mFwFrgFOBx4K0RsTOt4wpgCdlYMu+OiPUpfSHZTT2HAV+IiBUTrefWXU9y8fKvT3TxtrZs7nBXbvtY271zxdmjLh4R96Y75LdI2kB28+NdEbFC0nJgOdlowGcCs9NrHnAdMC8NvXIl0Ec2VMUWSesiYn/KcwnZgJB3kN1MeWdaZ7UyOk7vBNvkGN+bNVg9jjQuB7bl3n8c+GREvIrsTuMlKX0JsD+lfzLlQ9Ic4DzgtWQ7zuckHZaC0WfJdsg5wPkpr1mj/SIi7gWIiKfI2vdMYBGwJuVZQzZUNyl9bWQ2AsdImgGcAWyIiH0pUGwAFqZ5R0fExjSg5NqKdVUrw6wUJnWkIWkWcDbwEeAv0iH2G4A/TVnWkA1hfB3ZznBVSr8V+EzKvwi4OY2i+lNJOzg4RtKOiPhJKuvmlPfBydTZbDwk9QInkR0R9ETE7jTrUQ6OsDqT54+cOpjSRksfrJLOKGVU1msp2fD29PT0MDAwML4Na7ChoaEx67Rs7vCE1l3PbS1Sz7IoS10ne3rqU8B7gZFB7l4GPBERI60hvzM8twNFxLCkJ1P+mWTjyFBlmcodruhYM2aTlkb6/TLwnog4kB8wOCJCUkNvchqtjIhYBawC6Ovri7JdzC1y0Xaip0x3XjD6esfDF8LHb8JBQ9Kbgb0RsUUtfgxokV9dPUdO/JdNu+vWbR9ru0f71SbpcLKAcWNEjAz0uEfSjIjYnU4x7U3pu3j+cNuzUtouoL8ifSClz6qSf7QyzEphMkcapwF/KOkssmdPH0120foYSS9IRxv5nWFkxxpMDwl6KdkF8Vo7HKOkP0+RX12fvvF2PrG143oYF7Js7nBXbvtY2z3GL9brgW0RcU0ubR2wGFiR/t6eS78snUKdBzyZ/umvBz6a6wG1ALgiIvZJOiBpPtlpr4uAT49RhlkpTPhCeERcERGzIqKX7EL2tyPiAuBu4C0pW+WONdLl8C0pf6T08yS9MPW8mg38C9mjSWdLOiE97/q8lNes0Y4CLgTeIOm+9DqL7B/5myRtB96Y3kPW++knZMPLfx54F0BE7AM+RNaW7wGuTmmkPF9Iy/yYrOcUo5RhVgqN+Pn5PuBmSR8Gvk/2i43094vpQvc+siBARDwg6RayC9zDwKXpeddIugxYT9bldnVEPNCA+ppVGooI1Zh3emVC+vFzabXMEbEaWF0lfTNwYpX0x6uVYVYWdQkaETFAdq6W1Nvp1Cp5/gP4kxrLf4SsB1Zl+h1kv+LMzKwEfEe4mZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWWPc9ZMHM6q634il8y+YOT/jJfFZuPtIwM7PCHDTMzKwwBw0zMyvMQcPMzApz0DAzs8ImHDQkHSfpbkkPSnpA0uUpfZqkDZK2p79TU7okrZS0Q9L9kk7OrWtxyr9d0uJc+imStqZlVkqq9dxms7qStFrSXkk/zKU1vG3XKsOsLCZzpDEMLIuIOcB84FJJc4DlwF0RMRu4K70HOBOYnV5Lgesg20mAK4F5ZM8WvzK3o1wHXJJbbuEk6ms2HjdwaHtrRtuuVYZZKUw4aETE7oi4N00/BWwDZgKLgDUp2xrgnDS9CFgbmY3AMZJmAGcAGyJiX0TsBzYAC9O8oyNiY0QEsDa3LrOGiojvAPsqkpvRtmuVYVYKdbm5T1IvcBKwCeiJiN1p1qNAT5qeCTySW2wwpY2WPlgl3axVmtG2a5XxPJKWkh3V0NPTw8DAwAQ2p36WzR1+3vueIw9Nq5d6buvQ0FDLP7uiylLXSQcNSUcBXwbeExEH8pcdIiIkxWTLKFCHMXegRjbisuvWbR9ruyezAzajbY9WRkSsAlYB9PX1RX9/fyOrMqbKu7+XzR3mE1sbM+DEzgv667augYEBWv3ZFVWWuk7qW5V0OFnAuDEibkvJeyTNiIjd6TB8b0rfBRyXW3xWStsF9FekD6T0WVXyH6LIDvTpG29vWCMuu0buwGU21nZP4J9PM9p2rTLMSmEyvacEXA9si4hrcrPWASO9RBYDt+fSL0o9TeYDT6bD8PXAAklT00XCBcD6NO+ApPmprIty6zJrhWa07VplmJXCZH5+ngZcCGyVdF9Kez+wArhF0hLgYeDcNO8O4CxgB/AM8HaAiNgn6UPAPSnf1RExcgHyXWS9WI4E7kwvs4aTdBPZUcJ0SYNkvaCa0bZrlWFWChMOGhHxXaDWfROnV8kfwKU11rUaWF0lfTNw4kTraDZREXF+jVkNbdsR8Xi1MszKwneEm5lZYQ4aZmZWmIOGmZkV5qBhZmaFdV/nfTNra5WPli1i54qzG1CT7uQjDTMzK8xBw8zMCnPQMDOzwhw0zMysMAcNMzMrzEHDzMwKc9AwM7PCHDTMzKwwBw0zMyvMQcPMzArzMCJm9jwTGabDuoePNMzMrDAHDTMzK8xBw8zMCit90JC0UNJDknZIWt7q+pg1mtu8lVmpg4akw4DPAmcCc4DzJc1pba3MGsdt3squ1EEDOBXYERE/iYhngZuBRS2uk1kjuc1bqZW9y+1M4JHc+0FgXmUmSUuBpentkKSHqqxrOvBY3WvYBt7dpds+1nbr4zUXPb4R9Smonm2+ZcrW5kb5rktVzzE0uq6F2n3Zg0YhEbEKWDVaHkmbI6KvSVUqlW7d9k7e7iJtvpXa5bNvl3pCeepa9tNTu4Djcu9npTSzTuU2b6VW9qBxDzBb0gmSjgDOA9a1uE6lI2mnpDe2uh5WF27zVmqlDhoRMQxcBqwHtgG3RMQDE1xdaQ/lm2DMbZd0g6RnJQ1J2idpg6TfbkblGqjtvvM6t/lWapfPvl3qCSWpqyKi1XWwSZK0E3hHRHxrEuu4ARiMiA9KejHweeA3I2J+fWppVk6SXpCCtRVQ6iMNGx9JL5T0KUn/ll6fkvTC3Pz3Stqd5r1DUkh6VeV6IuIZ4O+BE5tZf7PJSKdp/4ek+yU9Lel6ST2S7pT0lKRvSZoqqTe1/SWSfgZ8u9V1bycOGp3lA8B84HeA15H1+f8gZHcZA38BvBF4FdBfayWSjgIuAL7f2Oqa1d0fA28Cfgv4A+BO4P3AsWT/796dy/t64DXAGU2uY1vriqDRrsMySDpO0t2SHpT0gKTLU/q0dM1hu6QNHPweLwD2AN8D7gJuBC5M894PHAF8FfgT4KqU/lpJW4E/ApZLegLYAUzNisrKkDS14RtcQdJhkr4v6R/T+xMkbUrf45fSheKRI6wvpfRNknpz67gipT8k6YxcetU2UasMq65oG21i+/l0ROyJiF3APwGbIuL7EfEfwFeAk4CRsd+vAn4dGCjb9y3pGEm3SvqRpG2SfreFn+nzdHzQaPNhGYaBZRExh+wI4tJU9+XAXRExmyw4vDTlnwVMA2aT3fi1FHi5pGlAH/BRsqOPK4Gn0jJXA5cAtwE7gfMj4jeArcA/5spoRbC9nOxi8IiPA5+MiFcB+4ElKX0JsD+lfzLlI31W5wGvBRYCn0uBaLQ2UasMq65oG21W+9mTm/55lfdzyH4UQXYTZVm/72uBb0TEb5OdNdhG6z7T5+n4oEEbD8sQEbsj4t40/RRZw5lJVv81Kdsa4MVp+j+A70VmI1kA2UN2+P0zYFpE7Ac2AOenZY5KeSELFOek6coyRtKbQtIs4GzgC+m9gDcAt1apU76utwKnp/yLgJsj4j8j4qdk/yxOpUabGKMMq2IcbbQMn+NUsruqb07vgxJ+35JeCvw+cD1ARDwbEU9Qks+0G4JGtWEZZraoLhOWTrmcBGwCeiJid5r1KHBYmt4LnCXpWEnTgZcA3yTb3k3A2yW9hiyQvC23/IgDHPxsKsvoqfc2jeFTwHuBX6X3LwOeyPVyyX+Pz33Haf6TKX+t775W+mhl2BjGaKPNbj/VvBXYThYsoLzf9wnAvwN/l07PfkHSFErymXZD0Gh76cL0l4H3RMSB/LzI+kyP7ATbgR8B95MdNTxF+rWS5q0E7gbeAzyc0n/FGCrKaDhJbwb2RsSWZpVpkzOONtoSqU09xcHTsmX2AuBk4LqIOAl4mopTUa38TLshaLT1sAySDifbGW+MiNtS8h5JM9L8GWSnWr5F9uv5toiYEREzgGeAn5I+g4j4WLpecRPZxfJfkfUqISIuBr7Bwc+msoy9Dd/Yg04D/lDZ/Sc3k51CuBY4RtLIeGn57/G57zjNfynwOLW/+1rpj49ShtVQsI02vP1ERG/+XqWIeFtEXJXenkZ2re9VZEexPweuoZzf9yDZPVOb0vtbyYJIK/fJ53RD0GjbYRnSOfbrgW0RcU1u1jpgcZpeDNyeS79ImfnAk+lwdj2wQNLbJPWQXRR+I/A14ICk+amsiyrWVa2MhouIKyJiVkT0kn1f346IC8iOkt5SpU75ur4l5Y+Ufl7qXXUC2T+Nf6FGm0jL1CrDqphAG22JCbSplomIR4FHJL06JZ0OPEhZPtOI6PgXcBbwr8CPgQ+0uj7jqPfvkR2C3g/cl15nkZ2LvYvslNO3yC5wA4isV9CPyU5P9eXW9d/IDnN/CQyRdT+cQdar6odpmc9wcJSAqmW04DPoJ+vFBfBKsn/6O4B/AF6Y0l+U3u9I81+ZW/4DadseAs4cq03UKsOv+rTRMryKtKlWv8jutdqcPtevkl3EL8Vn6mFEzMyssG44PWVmZnXSEQ9hyps+fXr09va2uhpVPf3000yZMqXV1RhTt9dzy5Ytj0XEsXVfcYM0us23S3tohG7a9qLtvuOCRm9vL5s3b251NaoaGBigv7+/1dUYU7fXU9LDY+cqj0a3+XZpD43QTdtetN379JTZoQ4fz3hKqbfayjR+0f2STh5ZkaTFKf92SYtz6adI2pqWWZl6IbVyzCazQjruSKOb9C7/+tiZqti54uw616QjLYuIeyW9BNiibGDIi8nG/lmhbJDD5cD7yMawmp1e84DrgHlpzK8ryXqoRVrPusiGcrmObMyvTcAdZN2g7+Tg+EKVZViT9S7/OsvmDnPxOPazbti3fKRhdqhfxPjGU1oErI3MRrIbxmaQjfm1ISL2xcExvxameUdHxMbIui+upSRjfpmNxUcaZqMoOJ7SeMe4mpmmK9MZpYzKeo2MYkxPTw8DAwPj27BxGBoaauj6y2rZ3GF6jsz+FtUNn5ODhlkNleMppcsOQDb2j6SG3uQ0WhkRsYr0zOi+vr5o5MXabroYnHdxOj31ia3F/03uvKC/cRUqCZ+eMqtinOMpjXeMq11pujJ9tDLMSsFBw6y6eo75NTX1gloArE/zSjfml1kRPj1ldqijyB6Tu1XSfSnt/cAK4BZJS8iGlj83zbuDbLylHWQjC78dICL2SfoQ2QCJAFdHxL40/S7gBuBIsl5Td6b0WmWYlYKDhtmhhiJCNeadXpmQekBdWi1zRKwGVldJ3wycWCX98WplmJWFT0+ZmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYb4j3Mw63kQfWGaH8pGGmZkV5qBhZmaF+fRUFxrrUL3ac5G74dnHZjY2H2mYmVlhDhpmZlbYmEFD0mpJeyX9MJc2TdIGSdvT36kpXZJWStoh6X5JJ+eWWZzyb5e0OJd+iqStaZmV6UlmNcswM7PWKXKkcQOwsCJtOXBXRMwG7krvAc4EZqfXUuA6yAIAcCUwDzgVuDIXBK4DLsktt3CMMszMrEXGDBoR8R1gX0XyImBNml4DnJNLXxuZjcAxkmYAZwAbImJfROwHNgAL07yjI2JjevrZ2op1VSvDzMxaZKK9p3oiYneafhToSdMzgUdy+QZT2mjpg1XSRyvjEJKWkh3Z0NPTw8DAwDg3pzmGhobqWrdlc4frtq68niMPXXcZP9N6f55mNrZJd7mNiJAU9ajMRMuIiFXAKoC+vr7o7+9vZHUmbGBggHrWrbJbbL0smzvMJ7Y+v2nsvKC/IWVNRr0/zzxJq4E3A3sj4sSUNg34EtAL7ATOjYj96TrctcBZwDPAxRFxb1pmMfDBtNoPR8SalH4K2anfI4E7gMtTO69aRkM20mwCJtp7ak86tUT6uzel7wKOy+WbldJGS59VJX20Msya4QZ8Lc/sEBMNGuuAkR5Qi4Hbc+kXpV5U84En0ymm9cACSVPTTrMAWJ/mHZA0P/1au6hiXdXKMGs4X8szq27M01OSbgL6gemSBsl+Oa0AbpG0BHgYODdlv4PsEH0H2WH62wEiYp+kDwH3pHxXR8TIDvkuDh6m35lejFKGWauU5lpeM6/jdcK1o4le/6t2fW807f45FTFm0IiI82vMOr1K3gAurbGe1cDqKumbgROrpD9erQyzMmj1tbxmXsdr5LWjZpno9b9q1/dGU8Zrf/XmO8LNivO1POt6HrCwJDzef1sYuc62gkOv5V0m6Wayi95PRsRuSeuBj+Yufi8Arkinaw+k636byK7lfXqMMsxKwUHDrApfyzOrzkHDrApfyzOrztc0zMysMAcNMzMrzEHDzMwKc9AwM7PCHDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMzMrDAHDTMzK8xBw8zMCvPYUw1Qa8TaZXOHG/ZcbzOzZnDQMLO24scItJZPT5mZWWE+0jAzq5OJHgXtXHF2nWvSOD7SMDOzwhw0zMysMAcNMzMrrPRBQ9JCSQ9J2iFpeavrY9ZobvNWZqW+EC7pMOCzwJuAQeAeSesi4sFm1cHd+6yZytDmzUZT6qABnArsiIifAEi6GVgEeAeyTtU1bd4/yNpT2YPGTOCR3PtBYF5lJklLgaXp7ZCkh5pQt3F7N0wHHmt1PcZSrZ76eIsqM7pGfZ7HN2CdRZWxzbdFu22EZu2zJdm/CrX7sgeNQiJiFbCq1fUYi6TNEdHX6nqMxfUsv2a2+W7+nLt522sp+4XwXcBxufezUppZp3Kbt1Ire9C4B5gt6QRJRwDnAetaXCezRnKbt1IrddCIiGHgMmA9sA24JSIeaG2tJqUhpxMk7ZT0xjqusvSn+pJ2qWdhJW3zHfc5j0M3b3tViohW18EmSdJO4B0R8a1W18XMOlupjzRsbJK+CLwC+JqkIUnvlRSSlkr6N0m7Jf1lLv+pkjZLOiBpj6RrcvP+QdKjkp6U9B1Jr23FNplZeTlotLmIuBD4GfAHEXEUcEua9V+B2cAC4H2501fXAtdGxNHAb+byA9yZlvl14F7gxsZvgZm1EweNJmjRsBB/HRFPR8RW4O+A81P6L4BXSZoeEUMRsTG3zF8B3wM2Aa8HXifppU2q76gkrZa0V9IPc2nTJG2QtD39ndrKOra7ap9xxfz+dBR6X3r9VbPr2AiSjpN0t6QHJT0g6fIqeSRpZdqH75d0civqWgYOGg2WGxbiTGAOcL6kOU0oOn+D2MPAy9P0EuC3gB9JukfSm3P1nAocBbwSOCHln96EuhZxA7CwIm05cFdEzAbuSu9t4m7g0M+40j9FxO+k19VNqFMzDAPLImIOMB+4tMo+eibZUfhsspsqr2tuFcvDQaPxnhsWIiKeBUaGhainar0Z8n39XwH8G0BEbI+I88lOQX0cuFXSFOBPgSOBPwZeCvSmZVXnuk5IRHwH2FeRvAhYk6bXAOc0tVIdpsZn3PEiYndE3JumnyLrtTazItsiYG1kNgLHSJrR5KqWgoNG41UbFqKyQU7WHrKjg7z/KenF6WL224EvAUh6m6RjI+JXwBMp76+Al5AFn9Vk1zO+Wuc6NkJPROxO048CPa2sTJf4XUk/kHRnJ3aUkNQLnER2ijavGftxW3DQ6AwfAz4o6QngLSnt/wE7yE7b/J+I+GZKXwg8IGmI7KL4eRHxc2AtMEB2cXw6hwahUous77j7jzfWvcDxEfE64NO0xw+LwiQdBXwZeE9EHGh1fcqqI8aeKrmGDwsREbcDt8Nzv5T+N7A6jU9UmfdtNdYxBJwx8l7SVcBQROyoZ13rbI+kGRGxO50q2NvqCnWy/D/SiLhD0udSh4q2H8xQ0uFkAePGiLitShYP75L4SKPx2mJYCElTJL1kZJqsq27VXjQlsg5YnKYXkwKnNYak35CkNH0q2f+Px1tbq8lL23Q9sC0irqmRbR1wUepFNR94MndqtKv4SKPBImJY0siwEIeRHQG0eliIanqAr6T/CS8A/j4ivtHaKh0k6SagH5guaRC4ElgB3CJpCVkPsXNbV8P2V+MzPhwgIv6G7NTnOyUNAz8nO7XZCacETwMuBLZKui+lvZ+sA8nItt8BnEV2yvcZsuuEXcnDiJiZWWE+PWVmZoV13Omp6dOnR29v7yHpTz/9NFOmTGl+hZqgU7etVdu1ZcuWxyLi2KYXbNYGOi5o9Pb2snnz5kPSBwYG6O/vb36FmqBTt61V2yXp4aYXatYmfHrKzMwK67gjDSuP3uVfH/cyO1ec3YCamFm9+EjDzMwKc9AwM7PCHDTMzKwwBw0zMytszKBR66lWtZ6aNtoTriQtTvm3S1qcSz9F0ta0zMrc+DZ+MpuZWYkUOdKo9VSrWk9Nq/qEK0nTyMaymUf2YKIrc0HgOuCS3HIjTw/zk9nMzEpkzKAxylOtaj01rdYTrs4ANkTEvojYD2wAFqZ5R0fExjT42dqKdfnJbGZmJTGu+zQqnmpV66lptZ5wNVr6YJV0Rimjsl5LyY5q6OnpYWBg4JA8Q0NDVdM7QVm3bdnc4XEvk9+Osm6XWTcrHDQqn2qVLjsA2VPTJDV0uNzRykgPG1oF0NfXF9WGnujUoTagvNt28URu7rug/7npsm6XWTcr1HuqxlOt9ow8WL3iqWm1nnA1WvqsKumjlWFmZi1QpPdUrada1XpqWq0nXK0HFkiami6ALwDWp3kHJM1PZV1UsS4/mc3MrCSKnJ6q9VSrWk9Nq/qEq4jYJ+lDZI8/Bbg6Ival6XcBNwBHAnemF6OUYWZmLTBm0IiI7wKqMfv0KvkDuLTGulYDq6ukbwZOrJL+eLUyzMysNXxHuJmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWWJHHva6WtFfSD3Np0yRtkLQ9/Z2a0iVppaQdku6XdHJumcUp/3ZJi3Ppp0jampZZmR75WrMMMzNrnSKPe70B+AywNpe2HLgrIlZIWp7evw84E5idXvOA64B5kqYBVwJ9QABbJK2LiP0pzyXAJrJHxS4ke9xrrTKsg/Uu//pz08vmDnNx7v1odq44u1FVMrOcMY80IuI7wL6K5EXAmjS9Bjgnl742MhuBYyTNAM4ANkTEvhREFiSxAAAD/ElEQVQoNgAL07yjI2Jjekzs2op1VSvDzMxapMiRRjU9EbE7TT8K9KTpmcAjuXyDKW209MEq6aOVcQhJS4GlAD09PQwMDBySZ2hoqGp6Jyjrti2bOzyp5XuOLL6OMm6/WSeaaNB4TkSEpKhHZSZaRkSsAlYB9PX1RX9//yF5BgYGqJbeCcq6bUVPLdWybO4wn9harInuvKB/UmWZWTET7T21J51aIv3dm9J3Acfl8s1KaaOlz6qSPloZZmbWIhMNGuuAkR5Qi4Hbc+kXpV5U84En0ymm9cACSVNTL6gFwPo074Ck+anX1EUV66pWhpmZtciYx/6SbgL6gemSBsl6Qa0AbpG0BHgYODdlvwM4C9gBPAO8HSAi9kn6EHBPynd1RIxcXH8XWQ+tI8l6Td2Z0muVYWZmLTJm0IiI82vMOr1K3gAurbGe1cDqKumbgROrpD9erQwzM2sd3xFuZmaFTbr3lHWH3kn2hDKzzuAjDTMzK8xBw8zMCnPQMDOzwhw0zMysMAcNMzMrzEHDzMwKc9AwM7PCHDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMzMrDAHDTMzK8xBw8zMCiv90OiSFgLXAocBX4iIFS2uUtvzMOdmNlGlPtKQdBjwWeBMYA5wvqQ5ra2VmVn3KvuRxqnAjoj4CYCkm4FFwIMtrVVJjBwxLJs7zMU+ejCzJih70JgJPJJ7PwjMq8wkaSmwNL0dkvRQlXVNBx6rew1L4N0dum3j2S59vK5FH1/XtZl1kLIHjUIiYhWwarQ8kjZHRF+TqtRUnbptnbpdZu2s1Nc0gF3Acbn3s1KamZm1QNmDxj3AbEknSDoCOA9Y1+I6mZl1rVKfnoqIYUmXAevJutyujogHJri6UU9ftblO3bZO3S6ztqWIaHUdzMysTZT99JSZmZWIg4aZmRXWFUFD0kJJD0naIWl5q+tTL5JWS9or6Yetrks9STpO0t2SHpT0gKTLW10nM8t0/DWNNBTJvwJvIrs58B7g/Iho+7vKJf0+MASsjYgTW12fepE0A5gREfdKegmwBTinE74zs3bXDUcazw1FEhHPAiNDkbS9iPgOsK/V9ai3iNgdEfem6aeAbWSjA5hZi3VD0Kg2FIn/AbUJSb3AScCm1tbEzKA7goa1KUlHAV8G3hMRB1pdHzPrjqDhoUjakKTDyQLGjRFxW6vrY2aZbggaHoqkzUgScD2wLSKuaXV9zOygjg8aETEMjAxFsg24ZRJDkZSKpJuAfwZeLWlQ0pJW16lOTgMuBN4g6b70OqvVlTKzLuhya2Zm9dPxRxpmZlY/DhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFfb/ARDAfwN8L2jyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_dataloader.dataset.data.hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder 2442385\n",
      "classifier 64015\n",
      "logP 131585\n",
      "mr 131585\n",
      "tpsa 131585\n",
      "0.5732572078704834 tensor(23.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08725333213806152 tensor(64.8558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08243489265441895 tensor(19.1111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0814046859741211 tensor(20.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08607745170593262 tensor(22.8063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0890200138092041 tensor(26.6114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0863807201385498 tensor(17.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08719348907470703 tensor(16.2195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08464670181274414 tensor(20.7388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08433723449707031 tensor(27.9163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "start validation\n",
      "logP tensor(1.2982, device='cuda:0')\n",
      "mr tensor(0.1813, device='cuda:0')\n",
      "tpsa tensor(0.3142, device='cuda:0')\n",
      "0.450620174407959 tensor(9.2434, device='cuda:0')\n",
      "logP tensor(1.4366, device='cuda:0')\n",
      "mr tensor(0.6314, device='cuda:0')\n",
      "tpsa tensor(0.6693, device='cuda:0')\n",
      "0.024400949478149414 tensor(10.9500, device='cuda:0')\n",
      "logP tensor(4.4847, device='cuda:0')\n",
      "mr tensor(0.0021, device='cuda:0')\n",
      "tpsa tensor(0.0462, device='cuda:0')\n",
      "0.031150341033935547 tensor(12.2235, device='cuda:0')\n",
      "logP tensor(5.9639, device='cuda:0')\n",
      "mr tensor(0.1521, device='cuda:0')\n",
      "tpsa tensor(0.2073, device='cuda:0')\n",
      "0.028149127960205078 tensor(14.1146, device='cuda:0')\n",
      "logP tensor(1.5642, device='cuda:0')\n",
      "mr tensor(0.0029, device='cuda:0')\n",
      "tpsa tensor(0.1357, device='cuda:0')\n",
      "0.02664351463317871 tensor(9.0007, device='cuda:0')\n",
      "logP tensor(7.2548, device='cuda:0')\n",
      "mr tensor(0.1845, device='cuda:0')\n",
      "tpsa tensor(0.2724, device='cuda:0')\n",
      "0.022295713424682617 tensor(15.2993, device='cuda:0')\n",
      "logP tensor(3.7322, device='cuda:0')\n",
      "mr tensor(0.0596, device='cuda:0')\n",
      "tpsa tensor(0.1874, device='cuda:0')\n",
      "0.023946523666381836 tensor(11.3439, device='cuda:0')\n",
      "logP tensor(4.7648, device='cuda:0')\n",
      "mr tensor(0.4789, device='cuda:0')\n",
      "tpsa tensor(0.4158, device='cuda:0')\n",
      "0.02362990379333496 tensor(13.4496, device='cuda:0')\n",
      "logP tensor(6.9834, device='cuda:0')\n",
      "mr tensor(0.1979, device='cuda:0')\n",
      "tpsa tensor(0.5913, device='cuda:0')\n",
      "0.026172637939453125 tensor(15.6417, device='cuda:0')\n",
      "logP tensor(5.2008, device='cuda:0')\n",
      "mr tensor(0.2511, device='cuda:0')\n",
      "tpsa tensor(0.3898, device='cuda:0')\n",
      "0.024233341217041016 tensor(13.6245, device='cuda:0')\n",
      "logP tensor(5.6274, device='cuda:0')\n",
      "mr tensor(0.2331, device='cuda:0')\n",
      "tpsa tensor(0.2692, device='cuda:0')\n",
      "0.039197683334350586 tensor(14.0630, device='cuda:0')\n",
      "logP tensor(4.6604, device='cuda:0')\n",
      "mr tensor(0.0055, device='cuda:0')\n",
      "tpsa tensor(0.0588, device='cuda:0')\n",
      "0.02346348762512207 tensor(12.1083, device='cuda:0')\n",
      "logP tensor(3.9673, device='cuda:0')\n",
      "mr tensor(0.1511, device='cuda:0')\n",
      "tpsa tensor(0.2789, device='cuda:0')\n",
      "0.024710416793823242 tensor(12.1825, device='cuda:0')\n",
      "logP tensor(4.5357, device='cuda:0')\n",
      "mr tensor(0.0008, device='cuda:0')\n",
      "tpsa tensor(0.0263, device='cuda:0')\n",
      "0.024407148361206055 tensor(12.0635, device='cuda:0')\n",
      "logP tensor(0.2589, device='cuda:0')\n",
      "mr tensor(0.4226, device='cuda:0')\n",
      "tpsa tensor(0.5544, device='cuda:0')\n",
      "0.02465987205505371 tensor(8.9765, device='cuda:0')\n",
      "logP tensor(3.1798, device='cuda:0')\n",
      "mr tensor(0.1165, device='cuda:0')\n",
      "tpsa tensor(0.2450, device='cuda:0')\n",
      "0.024125099182128906 tensor(11.2828, device='cuda:0')\n",
      "logP tensor(4.7320, device='cuda:0')\n",
      "mr tensor(0.0881, device='cuda:0')\n",
      "tpsa tensor(0.2037, device='cuda:0')\n",
      "0.025362014770507812 tensor(12.4732, device='cuda:0')\n",
      "logP tensor(4.2526, device='cuda:0')\n",
      "mr tensor(0.0853, device='cuda:0')\n",
      "tpsa tensor(0.1969, device='cuda:0')\n",
      "0.023013830184936523 tensor(11.9356, device='cuda:0')\n",
      "logP tensor(3.8630, device='cuda:0')\n",
      "mr tensor(0.0889, device='cuda:0')\n",
      "tpsa tensor(0.1390, device='cuda:0')\n",
      "0.029792308807373047 tensor(11.6164, device='cuda:0')\n",
      "logP tensor(1.2229, device='cuda:0')\n",
      "mr tensor(0.4066, device='cuda:0')\n",
      "tpsa tensor(0.4711, device='cuda:0')\n",
      "0.023325681686401367 tensor(10.1347, device='cuda:0')\n",
      "logP tensor(1.8466, device='cuda:0')\n",
      "mr tensor(0.6924, device='cuda:0')\n",
      "tpsa tensor(0.6556, device='cuda:0')\n",
      "0.023909568786621094 tensor(11.3096, device='cuda:0')\n",
      "logP tensor(4.2421, device='cuda:0')\n",
      "mr tensor(0.0870, device='cuda:0')\n",
      "tpsa tensor(0.1632, device='cuda:0')\n",
      "0.025063276290893555 tensor(12.0801, device='cuda:0')\n",
      "logP tensor(3.8328, device='cuda:0')\n",
      "mr tensor(0.0421, device='cuda:0')\n",
      "tpsa tensor(0.1120, device='cuda:0')\n",
      "0.026706933975219727 tensor(11.6798, device='cuda:0')\n",
      "logP tensor(4.6809, device='cuda:0')\n",
      "mr tensor(0.2232, device='cuda:0')\n",
      "tpsa tensor(0.4064, device='cuda:0')\n",
      "0.023333072662353516 tensor(12.8557, device='cuda:0')\n",
      "logP tensor(3.5344, device='cuda:0')\n",
      "mr tensor(0.1550, device='cuda:0')\n",
      "tpsa tensor(0.5959, device='cuda:0')\n",
      "0.025017499923706055 tensor(11.8654, device='cuda:0')\n",
      "logP tensor(2.3748, device='cuda:0')\n",
      "mr tensor(0.6322, device='cuda:0')\n",
      "tpsa tensor(0.6181, device='cuda:0')\n",
      "0.023227691650390625 tensor(11.7875, device='cuda:0')\n",
      "logP tensor(3.3716, device='cuda:0')\n",
      "mr tensor(0.0657, device='cuda:0')\n",
      "tpsa tensor(0.1837, device='cuda:0')\n",
      "0.025682449340820312 tensor(11.0237, device='cuda:0')\n",
      "logP tensor(8.4982, device='cuda:0')\n",
      "mr tensor(0.0443, device='cuda:0')\n",
      "tpsa tensor(0.1019, device='cuda:0')\n",
      "0.02463078498840332 tensor(16.1171, device='cuda:0')\n",
      "logP tensor(5.9580, device='cuda:0')\n",
      "mr tensor(0.0120, device='cuda:0')\n",
      "tpsa tensor(0.0725, device='cuda:0')\n",
      "0.02685260772705078 tensor(13.2445, device='cuda:0')\n",
      "logP tensor(3.5614, device='cuda:0')\n",
      "mr tensor(0.1513, device='cuda:0')\n",
      "tpsa tensor(0.2350, device='cuda:0')\n",
      "0.026622295379638672 tensor(11.6778, device='cuda:0')\n",
      "logP tensor(6.9030, device='cuda:0')\n",
      "mr tensor(0.3709, device='cuda:0')\n",
      "tpsa tensor(0.9175, device='cuda:0')\n",
      "\n",
      "tensor(381.4396, device='cuda:0') 1.8177778340665804 1.6330944305787856 0.38861371216274077 0.22778748743800287 0.5056376793692189 0.24329196003997214\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e16b58006bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-6079000383b8>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(data_loader, args)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mlist_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mlist_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mlist_logp_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlogp_mae\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mlist_logp_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlogp_std\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "result = experiment(train_dataloader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a      b   c\n",
      "0  1    0.1   3\n",
      "1  2  100.0  10\n",
      "2  3    4.0   5\n",
      "   a         b   c\n",
      "0  1  0.041393   3\n",
      "1  2  2.004321  10\n",
      "2  3  0.698970   5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "df = pd.DataFrame([[1,0.10000001,3], [2,100,10], [3,4,5]], columns=['a', 'b', 'c'])\n",
    "print(df)\n",
    "# df['b'] = (df['b']-df['b'].min())/(df['b'].max()-df['b'].min())\n",
    "df['b'] = np.log10(df['b']+1)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comet)",
   "language": "python",
   "name": "comet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
