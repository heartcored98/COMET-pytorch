{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from dataloader import *\n",
    "\n",
    "%matplotlib inline\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()   \n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "            \n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "            \n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2)) \n",
    "        attn_matrix = torch.mul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):    \n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gconv, Readout, BN1D, ResBlock, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, attn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.attn = attn\n",
    "        if self.attn is None:\n",
    "            self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        if self.attn is None:\n",
    "            x = self.fc(X)\n",
    "            x = torch.matmul(A, x)\n",
    "        else:\n",
    "            x = self.attn(X, A)            \n",
    "        return x, A\n",
    "    \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = self.relu(torch.sum(molvec, dim=1))\n",
    "        return molvec\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x): \n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn, use_attn, dp_rate, sc_type, n_attn_head=None):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.sc_type = sc_type\n",
    "        \n",
    "        attn = Attention(in_dim, out_dim, n_attn_head) if use_attn else None\n",
    "        self.gconv = GConv(in_dim, out_dim, attn)\n",
    "        \n",
    "        self.bn1 = BN1d(out_dim, use_bn)\n",
    "        self.dropout = nn.Dropout2d(p=dp_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if not self.sc_type in ['no', 'gsc', 'sc']:\n",
    "            raise Exception\n",
    "\n",
    "        if self.sc_type != 'no':\n",
    "            self.bn2 = BN1d(out_dim, use_bn)\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if in_dim != out_dim:\n",
    "                self.shortcut.add_module('shortcut', nn.Linear(in_dim, out_dim, bias=False))\n",
    "                \n",
    "        if self.sc_type == 'gsc':\n",
    "            self.g_fc1 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.g_fc2 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, A):     \n",
    "        x, A = self.gconv(X, A)\n",
    "\n",
    "        if self.sc_type == 'no': #no skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            return self.dropout(x), A\n",
    "        \n",
    "        elif self.sc_type == 'sc': # basic skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            x = x + self.shortcut(X)          \n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "        elif self.sc_type == 'gsc': # gated skip-connection\n",
    "            x = self.relu(self.bn1(x)) \n",
    "            x1 = self.g_fc1(self.shortcut(X))\n",
    "            x2 = self.g_fc2(x)\n",
    "            gate_coef = self.sigmoid(x1+x2)\n",
    "            x = torch.mul(x1, gate_coef) + torch.mul(x2, 1.0-gate_coef)\n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "    \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bs = args.batch_size\n",
    "        self.molvec_dim = args.molvec_dim\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train) \n",
    "        self.out_dim = args.out_dim\n",
    "        \n",
    "        list_gconvs = nn.ModuleList()\n",
    "        for i in range(args.num_layers):\n",
    "            if i==0:\n",
    "                list_gconvs.append(ResBlock(args.in_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "            else:\n",
    "                list_gconvs.append(ResBlock(self.out_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "                \n",
    "        self.gconvs = list_gconvs\n",
    "        \n",
    "        self.readout = Readout(self.out_dim, self.molvec_dim)\n",
    "    \n",
    "    def forward(self, input_X, A):   \n",
    "        x, A, molvec = self.encoder(input_X, A)\n",
    "        return x, A, molvec\n",
    "     \n",
    "    def encoder(self, input_X, A):\n",
    "        x = self._embed(input_X)\n",
    "        for i, module in enumerate(self.gconvs):\n",
    "            x, A = module(x, A)\n",
    "        molvec = self.readout(x)\n",
    "        return x, A, molvec\n",
    "    \n",
    "    def _embed(self, x):\n",
    "        embed_x = self.embedding(x[:,:,0])\n",
    "        x = torch.cat((embed_x.float(), x[:,:,1:].float()), 2)\n",
    "        return x \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train=False):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred_x, ground_x, vocab_size):\n",
    "    batch_size = ground_x.shape[0]\n",
    "    num_masking = ground_x.shape[1]\n",
    "    ground_x = ground_x.view(batch_size * num_masking, -1)\n",
    "    \n",
    "    symbol_loss = F.cross_entropy(pred_x[:,:vocab_size], ground_x[:, 0].detach())\n",
    "    degree_loss = F.cross_entropy(pred_x[:,vocab_size:vocab_size+6], ground_x[:,1:7].detach().max(dim=1)[1])\n",
    "    numH_loss = F.cross_entropy(pred_x[:,vocab_size+6:vocab_size+11], ground_x[:, 7:12].detach().max(dim=1)[1])\n",
    "    valence_loss = F.cross_entropy(pred_x[:,vocab_size+11:vocab_size+17], ground_x[:,12:18].detach().max(dim=1)[1])\n",
    "    isarom_loss = F.binary_cross_entropy(torch.sigmoid(pred_x[:,-1]), ground_x[:,-1].detach().float())\n",
    "    total_loss = symbol_loss + degree_loss + numH_loss + valence_loss + isarom_loss\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier & Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, molvec_dim, vocab_size, dropout_rate=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.vs = vocab_size\n",
    "    \n",
    "        self.fc1 = nn.Linear(self.molvec_dim + self.out_dim, args.in_dim)\n",
    "        self.fc2 = nn.Linear(self.in_dim, args.in_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.param_initializer()\n",
    "        \n",
    "    def forward(self, X, molvec, idx_M):\n",
    "        batch_size = X.shape[0]\n",
    "        #print('idx_M', idx_M.shape)\n",
    "        num_masking = idx_M.shape[1]\n",
    "        probs_atom = list()\n",
    "        probs_degree = list()\n",
    "        probs_numH = list()\n",
    "        probs_valence = list()\n",
    "        probs_isarom = list()\n",
    "        \n",
    "        molvec = torch.unsqueeze(molvec, 1)\n",
    "        molvec = molvec.expand(batch_size, num_masking, molvec.shape[-1])\n",
    "        \n",
    "        list_concat_x = list()\n",
    "        for i in range(batch_size):\n",
    "            target_x = torch.index_select(X[i], 0, idx_M[i])\n",
    "            concat_x = torch.cat((target_x, molvec[i]), dim=1)\n",
    "            list_concat_x.append(concat_x)\n",
    "            \n",
    "        concat_x = torch.stack(list_concat_x)\n",
    "        pred_x = self.classify(concat_x)\n",
    "        pred_x = pred_x.view(batch_size * num_masking, -1)\n",
    "        return pred_x\n",
    "    \n",
    "    def classify(self, concat_x):\n",
    "        x = self.relu(self.fc1(concat_x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "    \n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, molvec_dim, dropout_rate):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.reg_fc1 = nn.Linear(self.molvec_dim, self.molvec_dim//2)\n",
    "        self.reg_fc2 = nn.Linear(self.molvec_dim//2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, molvec):\n",
    "        x = self.relu(self.reg_fc1(molvec))\n",
    "        x = self.reg_fc2(x)\n",
    "        return torch.squeeze(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, data_loader, optimizer, args, **kwargs):\n",
    "    t = time.time()\n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        for _, model in models.items():\n",
    "            model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "        input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "        A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "        mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "        logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "        ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "        idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "        \n",
    "        \n",
    "        # Encoding Molecule\n",
    "        X, A, molvec = models['encoder'](input_X, A)\n",
    "        pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "        \n",
    "        \n",
    "        # Compute Mask Task Loss & Property Regression Loss\n",
    "        train_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "        loss_log = {'mask_loss':train_loss}\n",
    "          \n",
    "        if args.train_logp:\n",
    "            pred_logp = models['logP'](molvec)\n",
    "            logP_loss = reg_loss(pred_logp, logp)\n",
    "            loss_log.update({'logP_loss': logP_loss})\n",
    "            train_loss += logP_loss\n",
    "        if args.train_mr:\n",
    "            pred_mr = models['mr'](molvec)\n",
    "            mr_loss = reg_loss(pred_mr, mr)\n",
    "            loss_log.update({'mr_loss': mr_loss})\n",
    "            train_loss += mr_loss\n",
    "        if args.train_tpsa:\n",
    "            pred_tpsa = models['tpsa'](molvec)\n",
    "            tpsa_loss = reg_loss(pred_tpsa, tpsa)\n",
    "            loss_log.update({'tpsa_loss': tpsa_loss})\n",
    "            train_loss += tpsa_loss\n",
    "                \n",
    "        writer.add_scalars('data/loss_group', loss_log, cnt_iter)\n",
    "\n",
    "        #print(train_loss)\n",
    "        epoch_train_loss += train_loss\n",
    "        list_train_loss.append({'epoch':batch_idx/len(data_loader)+kwargs['epoch'], 'train_loss':train_loss})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt_iter += 1   \n",
    "        \n",
    "        \n",
    "        if cnt_iter % args.save_every:\n",
    "            pass\n",
    "        \n",
    "        if cnt_iter % args.validate_every == 0:\n",
    "            optimizer.zero_grad()\n",
    "            validate(models, data_loader, args)\n",
    "            \n",
    "        print(time.time()-t, train_loss)\n",
    "        t = time.time()\n",
    "        \n",
    "    return models, list_train_loss\n",
    "\n",
    "\n",
    "def validate(models, data_loader, args, **kwargs):\n",
    "\n",
    "    t = time.time()\n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "    \n",
    "    list_logp, list_pred_logp = [], []\n",
    "    list_mr, list_pred_mr = [], []\n",
    "    list_tpsa, list_pred_tpsa = [], []\n",
    "    logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Initialization Model with Evaluation Mode\n",
    "    for _, model in models.items():\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "            input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "            A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "            mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "            logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "            ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "            idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "\n",
    "            # Encoding Molecule\n",
    "            X, A, molvec = models['encoder'](input_X, A)\n",
    "            pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "\n",
    "            # Compute Mask Task Loss & Property Regression Loss\n",
    "            val_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "\n",
    "            if args.train_logp:\n",
    "                pred_logp = models['logP'](molvec)\n",
    "                val_loss += reg_loss(pred_logp, logp)\n",
    "                list_logp += logp.cpu().detach().numpy().tolist()\n",
    "                list_pred_logp += pred_logp.cpu().detach().numpy().tolist()\n",
    "            \n",
    "            if args.train_mr:\n",
    "                pred_mr = models['mr'](molvec)\n",
    "                val_loss += reg_loss(pred_mr, mr)\n",
    "                list_mr += mr.cpu().detach().numpy().tolist()\n",
    "                list_pred_mr += pred_mr.cpu().detach().numpy().tolist()\n",
    "            \n",
    "            if args.train_tpsa:\n",
    "                pred_tpsa = models['tpsa'](molvec)\n",
    "                val_loss += reg_loss(pred_tpsa, tpsa)\n",
    "                list_tpsa += tpsa.cpu().detach().numpy().tolist()\n",
    "                list_pred_tpsa += pred_tpsa.cpu().detach().numpy().tolist()\n",
    "\n",
    "            #print(val_loss)\n",
    "            epoch_val_loss += val_loss\n",
    "            cnt_iter += 1   \n",
    "            if cnt_iter > 30:\n",
    "                break\n",
    "\n",
    "            if cnt_iter % args.save_every:\n",
    "                pass\n",
    "            print(time.time()-t, val_loss)\n",
    "            t = time.time()\n",
    "    \n",
    "    # Calculate overall MAE and STD value      \n",
    "    if args.train_logp:\n",
    "        logp_mae = mean_absolute_error(list_logp, list_pred_logp)\n",
    "        logp_std = np.std(np.array(list_logp)-np.array(list_pred_logp))\n",
    "        \n",
    "    if args.train_mr:\n",
    "        mr_mae = mean_absolute_error(list_mr, list_pred_mr)\n",
    "        mr_std = np.std(np.array(list_mr)-np.array(list_pred_mr))\n",
    "        \n",
    "    if args.train_tpsa:\n",
    "        tpsa_mae = mean_absolute_error(list_tpsa, list_pred_tpsa)\n",
    "        tpsa_std = np.std(np.array(list_tpsa)-np.array(list_pred_tpsa))\n",
    "                \n",
    "    torch.cuda.empty_cache()\n",
    "    print()\n",
    "    print(epoch_val_loss, logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std)\n",
    "    return epoch_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, model_name, epoch):\n",
    "    filename= '{}_{}_ckpt.pth'.format(model_name, epoch)\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "\n",
    "def experiment(train_dataloader, val_dataloader, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    # Construct Model\n",
    "    encoder = Encoder(args)\n",
    "    classifier = Classifier(args.in_dim, args.out_dim, args.molvec_dim, args.vocab_size, args.dp_rate)\n",
    "    models = {'encoder': encoder, 'classifier': classifier}\n",
    "    if args.train_logp:\n",
    "        models.update({'logP': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_mr:\n",
    "        models.update({'mr': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_tpsa:\n",
    "        models.update({'tpsa': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = list()\n",
    "    for key, model in models.items():\n",
    "        model.to(args.device)\n",
    "        trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        print(key, sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Initialize Data Logger\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_logp_mae = list()\n",
    "    list_logp_std = list()\n",
    "    list_mr_mae = list()\n",
    "    list_mr_std = list()\n",
    "    list_tpsa_mae = list()\n",
    "    list_tpsa_std = list()\n",
    "    \n",
    "    tot_iter = 0\n",
    "    args.best_mae = 10000\n",
    "    \n",
    "    # Train Model\n",
    "    train(models, data_loader, optimizer, args, **{'epoch':epoch})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    list_train_loss += train_losses\n",
    "    list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "    list_logp_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "    list_logp_std.append({'epoch':epoch, 'std':logp_std})\n",
    "    list_mr_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "    list_mr_std.append({'epoch':epoch, 'std':logp_std})\n",
    "    list_tpsa_mae.append({'epoch':epoch, 'mae':tpsa_mae})\n",
    "    list_tpsa_std.append({'epoch':epoch, 'std':tpsa_std})\n",
    "\n",
    "    if args.best_mae > mae or epoch==0:\n",
    "        args.best_epoch = epoch\n",
    "        args.best_mae = mae\n",
    "        args.best_std = std\n",
    "        #args.best_true_y = true_y\n",
    "        #args.best_pred_y = pred_y\n",
    "\n",
    "    if total_iter % args.save_every == 0:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'encoder': models['encoder'],\n",
    "            'encoder_state_dict': models['encoder'].state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "             })\n",
    "\n",
    "    total_iter += 1\n",
    "\n",
    "          \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.logp_maes = list_logp_mae\n",
    "    args.logp_stds = list_logp_std\n",
    "    args.tpsa_maes = list_tpsa_mae\n",
    "    args.tpsa_stds = list_tpsa_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "def make_model_comment(args, prior_keyword=['num_layers', 'out_dim', \n",
    "                                       'molvec_dim', 'sc_type',\n",
    "                                       'use_attn', 'n_attn_heads',\n",
    "                                       'use_bn', 'emb_train', \n",
    "                                       'train_logp', 'train_mr', \n",
    "                                       'train_tpsa', 'optim', \n",
    "                                       'lr', 'l2_coef', \n",
    "                                       'dp_rate', 'batch_size']):\n",
    "    model_name = \"_\"\n",
    "    dict_args = vars(args)\n",
    "    if 'bar' in dict_args:\n",
    "        del dict_args['bar']\n",
    "    for keyword in prior_keyword:\n",
    "        value = str(dict_args[keyword])\n",
    "        if value.isdigit():\n",
    "            try:\n",
    "                value = int(value)\n",
    "                model_name += keyword + ':{}_'.format(dict_args[keyword])\n",
    "            except:\n",
    "                model_name += keyword + ':{:.2E}_'.format(Decimal(dict_args[keyword]))\n",
    "        else:\n",
    "            model_name += keyword + ':{}_'.format(value)\n",
    "    return model_name[:215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "##### SIZE #####\n",
    "args.vocab_size = 41\n",
    "args.in_dim = 59\n",
    "args.out_dim = 512\n",
    "args.molvec_dim = 512\n",
    "\n",
    "\n",
    "##### MODEL #####\n",
    "args.num_layers = 8\n",
    "args.use_attn = True\n",
    "args.n_attn_heads = 8\n",
    "args.use_bn = True\n",
    "args.sc_type = 'sc'\n",
    "args.emb_train = True\n",
    "args.train_logp = True\n",
    "args.train_mr = True\n",
    "args.train_tpsa = True\n",
    "\n",
    "##### HYPERPARAMETERS #####\n",
    "args.optim = 'ADAM'\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.dp_rate = 0.1\n",
    "\n",
    "##### EXP #####\n",
    "args.epoch = 2\n",
    "args.batch_size = 16\n",
    "args.test_batch_size = 16\n",
    "args.save_every = 50\n",
    "args.validate_every = 30\n",
    "\n",
    "##### DEVICE #####\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "##### LOGGING #####\n",
    "args.log_path = 'runs'\n",
    "model_name = make_model_comment(args)\n",
    "writer = SummaryWriter(join(args.log_path, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/processed_zinc_smiles/data_xs/train'\n",
    "val_dataset_path = './dataset/processed_zinc_smiles/data_xs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)\n",
    "list_vals = get_dir_files(val_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26508\n",
      "6696\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = zincDataLoader(join(train_dataset_path, list_trains[0]),\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=True,\n",
    "                                  num_workers=8)\n",
    "\n",
    "val_dataloader = zincDataLoader(join(val_dataset_path, list_vals[0]),\n",
    "                                  batch_size=args.test_batch_size,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=False,\n",
    "                                  num_workers=8)\n",
    "print(len(train_dataloader))              \n",
    "print(len(val_dataloader))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder 2442385\n",
      "classifier 64015\n",
      "logP 131585\n",
      "mr 131585\n",
      "tpsa 131585\n",
      "0.5732572078704834 tensor(23.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08725333213806152 tensor(64.8558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08243489265441895 tensor(19.1111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0814046859741211 tensor(20.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08607745170593262 tensor(22.8063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0890200138092041 tensor(26.6114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.0863807201385498 tensor(17.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08719348907470703 tensor(16.2195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08464670181274414 tensor(20.7388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.08433723449707031 tensor(27.9163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "start validation\n",
      "logP tensor(1.2982, device='cuda:0')\n",
      "mr tensor(0.1813, device='cuda:0')\n",
      "tpsa tensor(0.3142, device='cuda:0')\n",
      "0.450620174407959 tensor(9.2434, device='cuda:0')\n",
      "logP tensor(1.4366, device='cuda:0')\n",
      "mr tensor(0.6314, device='cuda:0')\n",
      "tpsa tensor(0.6693, device='cuda:0')\n",
      "0.024400949478149414 tensor(10.9500, device='cuda:0')\n",
      "logP tensor(4.4847, device='cuda:0')\n",
      "mr tensor(0.0021, device='cuda:0')\n",
      "tpsa tensor(0.0462, device='cuda:0')\n",
      "0.031150341033935547 tensor(12.2235, device='cuda:0')\n",
      "logP tensor(5.9639, device='cuda:0')\n",
      "mr tensor(0.1521, device='cuda:0')\n",
      "tpsa tensor(0.2073, device='cuda:0')\n",
      "0.028149127960205078 tensor(14.1146, device='cuda:0')\n",
      "logP tensor(1.5642, device='cuda:0')\n",
      "mr tensor(0.0029, device='cuda:0')\n",
      "tpsa tensor(0.1357, device='cuda:0')\n",
      "0.02664351463317871 tensor(9.0007, device='cuda:0')\n",
      "logP tensor(7.2548, device='cuda:0')\n",
      "mr tensor(0.1845, device='cuda:0')\n",
      "tpsa tensor(0.2724, device='cuda:0')\n",
      "0.022295713424682617 tensor(15.2993, device='cuda:0')\n",
      "logP tensor(3.7322, device='cuda:0')\n",
      "mr tensor(0.0596, device='cuda:0')\n",
      "tpsa tensor(0.1874, device='cuda:0')\n",
      "0.023946523666381836 tensor(11.3439, device='cuda:0')\n",
      "logP tensor(4.7648, device='cuda:0')\n",
      "mr tensor(0.4789, device='cuda:0')\n",
      "tpsa tensor(0.4158, device='cuda:0')\n",
      "0.02362990379333496 tensor(13.4496, device='cuda:0')\n",
      "logP tensor(6.9834, device='cuda:0')\n",
      "mr tensor(0.1979, device='cuda:0')\n",
      "tpsa tensor(0.5913, device='cuda:0')\n",
      "0.026172637939453125 tensor(15.6417, device='cuda:0')\n",
      "logP tensor(5.2008, device='cuda:0')\n",
      "mr tensor(0.2511, device='cuda:0')\n",
      "tpsa tensor(0.3898, device='cuda:0')\n",
      "0.024233341217041016 tensor(13.6245, device='cuda:0')\n",
      "logP tensor(5.6274, device='cuda:0')\n",
      "mr tensor(0.2331, device='cuda:0')\n",
      "tpsa tensor(0.2692, device='cuda:0')\n",
      "0.039197683334350586 tensor(14.0630, device='cuda:0')\n",
      "logP tensor(4.6604, device='cuda:0')\n",
      "mr tensor(0.0055, device='cuda:0')\n",
      "tpsa tensor(0.0588, device='cuda:0')\n",
      "0.02346348762512207 tensor(12.1083, device='cuda:0')\n",
      "logP tensor(3.9673, device='cuda:0')\n",
      "mr tensor(0.1511, device='cuda:0')\n",
      "tpsa tensor(0.2789, device='cuda:0')\n",
      "0.024710416793823242 tensor(12.1825, device='cuda:0')\n",
      "logP tensor(4.5357, device='cuda:0')\n",
      "mr tensor(0.0008, device='cuda:0')\n",
      "tpsa tensor(0.0263, device='cuda:0')\n",
      "0.024407148361206055 tensor(12.0635, device='cuda:0')\n",
      "logP tensor(0.2589, device='cuda:0')\n",
      "mr tensor(0.4226, device='cuda:0')\n",
      "tpsa tensor(0.5544, device='cuda:0')\n",
      "0.02465987205505371 tensor(8.9765, device='cuda:0')\n",
      "logP tensor(3.1798, device='cuda:0')\n",
      "mr tensor(0.1165, device='cuda:0')\n",
      "tpsa tensor(0.2450, device='cuda:0')\n",
      "0.024125099182128906 tensor(11.2828, device='cuda:0')\n",
      "logP tensor(4.7320, device='cuda:0')\n",
      "mr tensor(0.0881, device='cuda:0')\n",
      "tpsa tensor(0.2037, device='cuda:0')\n",
      "0.025362014770507812 tensor(12.4732, device='cuda:0')\n",
      "logP tensor(4.2526, device='cuda:0')\n",
      "mr tensor(0.0853, device='cuda:0')\n",
      "tpsa tensor(0.1969, device='cuda:0')\n",
      "0.023013830184936523 tensor(11.9356, device='cuda:0')\n",
      "logP tensor(3.8630, device='cuda:0')\n",
      "mr tensor(0.0889, device='cuda:0')\n",
      "tpsa tensor(0.1390, device='cuda:0')\n",
      "0.029792308807373047 tensor(11.6164, device='cuda:0')\n",
      "logP tensor(1.2229, device='cuda:0')\n",
      "mr tensor(0.4066, device='cuda:0')\n",
      "tpsa tensor(0.4711, device='cuda:0')\n",
      "0.023325681686401367 tensor(10.1347, device='cuda:0')\n",
      "logP tensor(1.8466, device='cuda:0')\n",
      "mr tensor(0.6924, device='cuda:0')\n",
      "tpsa tensor(0.6556, device='cuda:0')\n",
      "0.023909568786621094 tensor(11.3096, device='cuda:0')\n",
      "logP tensor(4.2421, device='cuda:0')\n",
      "mr tensor(0.0870, device='cuda:0')\n",
      "tpsa tensor(0.1632, device='cuda:0')\n",
      "0.025063276290893555 tensor(12.0801, device='cuda:0')\n",
      "logP tensor(3.8328, device='cuda:0')\n",
      "mr tensor(0.0421, device='cuda:0')\n",
      "tpsa tensor(0.1120, device='cuda:0')\n",
      "0.026706933975219727 tensor(11.6798, device='cuda:0')\n",
      "logP tensor(4.6809, device='cuda:0')\n",
      "mr tensor(0.2232, device='cuda:0')\n",
      "tpsa tensor(0.4064, device='cuda:0')\n",
      "0.023333072662353516 tensor(12.8557, device='cuda:0')\n",
      "logP tensor(3.5344, device='cuda:0')\n",
      "mr tensor(0.1550, device='cuda:0')\n",
      "tpsa tensor(0.5959, device='cuda:0')\n",
      "0.025017499923706055 tensor(11.8654, device='cuda:0')\n",
      "logP tensor(2.3748, device='cuda:0')\n",
      "mr tensor(0.6322, device='cuda:0')\n",
      "tpsa tensor(0.6181, device='cuda:0')\n",
      "0.023227691650390625 tensor(11.7875, device='cuda:0')\n",
      "logP tensor(3.3716, device='cuda:0')\n",
      "mr tensor(0.0657, device='cuda:0')\n",
      "tpsa tensor(0.1837, device='cuda:0')\n",
      "0.025682449340820312 tensor(11.0237, device='cuda:0')\n",
      "logP tensor(8.4982, device='cuda:0')\n",
      "mr tensor(0.0443, device='cuda:0')\n",
      "tpsa tensor(0.1019, device='cuda:0')\n",
      "0.02463078498840332 tensor(16.1171, device='cuda:0')\n",
      "logP tensor(5.9580, device='cuda:0')\n",
      "mr tensor(0.0120, device='cuda:0')\n",
      "tpsa tensor(0.0725, device='cuda:0')\n",
      "0.02685260772705078 tensor(13.2445, device='cuda:0')\n",
      "logP tensor(3.5614, device='cuda:0')\n",
      "mr tensor(0.1513, device='cuda:0')\n",
      "tpsa tensor(0.2350, device='cuda:0')\n",
      "0.026622295379638672 tensor(11.6778, device='cuda:0')\n",
      "logP tensor(6.9030, device='cuda:0')\n",
      "mr tensor(0.3709, device='cuda:0')\n",
      "tpsa tensor(0.9175, device='cuda:0')\n",
      "\n",
      "tensor(381.4396, device='cuda:0') 1.8177778340665804 1.6330944305787856 0.38861371216274077 0.22778748743800287 0.5056376793692189 0.24329196003997214\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e16b58006bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-6079000383b8>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(data_loader, args)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mlist_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mlist_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mlist_logp_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlogp_mae\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mlist_logp_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlogp_std\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "result = experiment(train_dataloader, val_dataloader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a      b   c\n",
      "0  1    0.1   3\n",
      "1  2  100.0  10\n",
      "2  3    4.0   5\n",
      "   a         b   c\n",
      "0  1  0.041393   3\n",
      "1  2  2.004321  10\n",
      "2  3  0.698970   5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "df = pd.DataFrame([[1,0.10000001,3], [2,100,10], [3,4,5]], columns=['a', 'b', 'c'])\n",
    "print(df)\n",
    "# df['b'] = (df['b']-df['b'].min())/(df['b'].max()-df['b'].min())\n",
    "df['b'] = np.log10(df['b']+1)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comet)",
   "language": "python",
   "name": "comet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
