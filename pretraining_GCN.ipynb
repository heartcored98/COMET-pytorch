{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from dataloader import *\n",
    "\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()   \n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "            \n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "            \n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2)) \n",
    "        attn_matrix = torch.matmul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):    \n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gconv, Readout, BN1D, ResBlock, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, attn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.attn = attn\n",
    "        if self.attn is None:\n",
    "            self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        if self.attn is None:\n",
    "            x = self.fc(X)\n",
    "            x = torch.matmul(A, x)\n",
    "        else:\n",
    "            x = self.attn(X, A)            \n",
    "        return x, A\n",
    "    \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = self.relu(torch.sum(molvec, dim=1))\n",
    "        return molvec\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x): \n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn, use_attn, dp_rate, sc_type, n_attn_head=None):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.sc_type = sc_type\n",
    "        \n",
    "        attn = Attention(in_dim, out_dim, n_attn_head) if use_attn else None\n",
    "        self.gconv = GConv(in_dim, out_dim, attn)\n",
    "        \n",
    "        self.bn1 = BN1d(out_dim, use_bn)\n",
    "        self.dropout = nn.Dropout2d(p=dp_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if not self.sc_type in ['no', 'gsc', 'sc']:\n",
    "            raise Exception\n",
    "\n",
    "        if self.sc_type != 'no':\n",
    "            self.bn2 = BN1d(out_dim, use_bn)\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if in_dim != out_dim:\n",
    "                self.shortcut.add_module('shortcut', nn.Linear(in_dim, out_dim, bias=False))\n",
    "                \n",
    "        if self.sc_type == 'gsc':\n",
    "            self.g_fc1 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.g_fc2 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, A):     \n",
    "        x, A = self.gconv(X, A)\n",
    "\n",
    "        if self.sc_type == 'no': #no skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            return self.dropout(x), A\n",
    "        \n",
    "        elif self.sc_type == 'sc': # basic skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            x = x + self.shortcut(X)          \n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "        elif self.sc_type == 'gsc': # gated skip-connection\n",
    "            x = self.relu(self.bn1(x)) \n",
    "            x1 = self.g_fc1(self.shortcut(X))\n",
    "            x2 = self.g_fc2(x)\n",
    "            gate_coef = self.sigmoid(x1+x2)\n",
    "            x = torch.mul(x1, gate_coef) + torch.mul(x2, 1.0-gate_coef)\n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "    \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bs = args.batch_size\n",
    "        self.molvec_dim = args.molvec_dim\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train) \n",
    "        self.out_dim = args.out_dim\n",
    "        \n",
    "        list_gconvs = nn.ModuleList()\n",
    "        for i in range(args.num_layers):\n",
    "            if i==0:\n",
    "                list_gconvs.append(ResBlock(args.in_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "            else:\n",
    "                list_gconvs.append(ResBlock(self.out_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "                \n",
    "        self.gconvs = list_gconvs\n",
    "        \n",
    "        self.readout = Readout(self.out_dim, self.molvec_dim)\n",
    "    \n",
    "    def forward(self, input_X, A):   \n",
    "        x, A, molvec = self.encoder(input_X, A)\n",
    "        return x, A, molvec\n",
    "     \n",
    "    def encoder(self, input_X, A):\n",
    "        x = self._embed(input_X)\n",
    "        for i, module in enumerate(self.gconvs):\n",
    "            x, A = module(x, A)\n",
    "        molvec = self.readout(x)\n",
    "        return x, A, molvec\n",
    "    \n",
    "    def _embed(self, x):\n",
    "        embed_x = self.embedding(x[:,:,0])\n",
    "        x = torch.cat((embed_x.float(), x[:,:,1:].float()), 2)\n",
    "        return x \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train=False):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred_x, ground_x, vocab_size):\n",
    "    batch_size = ground_x.shape[0]\n",
    "    num_masking = ground_x.shape[1]\n",
    "    ground_x = ground_x.view(batch_size * num_masking, -1)\n",
    "    \n",
    "    symbol_loss = F.cross_entropy(pred_x[:,:vocab_size], ground_x[:, 0].detach())\n",
    "    degree_loss = F.cross_entropy(pred_x[:,vocab_size:vocab_size+6], ground_x[:,1:7].detach().max(dim=1)[1])\n",
    "    numH_loss = F.cross_entropy(pred_x[:,vocab_size+6:vocab_size+11], ground_x[:, 7:12].detach().max(dim=1)[1])\n",
    "    valence_loss = F.cross_entropy(pred_x[:,vocab_size+11:vocab_size+17], ground_x[:,12:18].detach().max(dim=1)[1])\n",
    "    isarom_loss = F.binary_cross_entropy(torch.sigmoid(pred_x[:,-1]), ground_x[:,-1].detach().float())\n",
    "    total_loss = symbol_loss + degree_loss + numH_loss + valence_loss + isarom_loss\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier & Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, molvec_dim, vocab_size, dropout_rate=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.vs = vocab_size\n",
    "    \n",
    "        self.fc1 = nn.Linear(self.molvec_dim + self.out_dim, args.in_dim)\n",
    "        self.fc2 = nn.Linear(self.in_dim, args.in_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.param_initializer()\n",
    "        \n",
    "    def forward(self, X, molvec, idx_M):\n",
    "        batch_size = X.shape[0]\n",
    "        #print('idx_M', idx_M.shape)\n",
    "        num_masking = idx_M.shape[1]\n",
    "        probs_atom = list()\n",
    "        probs_degree = list()\n",
    "        probs_numH = list()\n",
    "        probs_valence = list()\n",
    "        probs_isarom = list()\n",
    "        \n",
    "        molvec = torch.unsqueeze(molvec, 1)\n",
    "        molvec = molvec.expand(batch_size, num_masking, molvec.shape[-1])\n",
    "        \n",
    "        list_concat_x = list()\n",
    "        for i in range(batch_size):\n",
    "            target_x = torch.index_select(X[i], 0, idx_M[i])\n",
    "            concat_x = torch.cat((target_x, molvec[i]), dim=1)\n",
    "            list_concat_x.append(concat_x)\n",
    "            \n",
    "        concat_x = torch.stack(list_concat_x)\n",
    "        pred_x = self.classify(concat_x)\n",
    "        pred_x = pred_x.view(batch_size * num_masking, -1)\n",
    "        return pred_x\n",
    "    \n",
    "    def classify(self, concat_x):\n",
    "        x = self.relu(self.fc1(concat_x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "    \n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, molvec_dim, dropout_rate):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.reg_fc1 = nn.Linear(self.molvec_dim, self.molvec_dim//2)\n",
    "        self.reg_fc2 = nn.Linear(self.molvec_dim//2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, molvec):\n",
    "        x = self.relu(self.reg_fc1(molvec))\n",
    "        x = self.reg_fc2(x)\n",
    "        return torch.squeeze(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, data_loader, optimizer, args, **kwargs):\n",
    "        \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "        input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "        A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "        mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "        logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "        ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "        idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "        \n",
    "        for _, model in models.items():\n",
    "            model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoding Molecule\n",
    "        X, A, molvec = models['encoder'](input_X, A)\n",
    "        pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "        \n",
    "        # Compute Mask Task Loss & Property Regression Loss\n",
    "        train_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "                        \n",
    "        if args.train_logp:\n",
    "            pred_logp = models['logP'](molvec)\n",
    "            train_loss += reg_loss(pred_logp, logp)\n",
    "        if args.train_mr:\n",
    "            pred_mr = models['mr'](molvec)\n",
    "            train_loss += reg_loss(pred_mr, mr)\n",
    "        if args.train_tpsa:\n",
    "            pred_tpsa = models['tpsa'](molvec)\n",
    "            train_loss += reg_loss(pred_tpsa, tpsa)\n",
    "        \n",
    "        #print(train_loss)\n",
    "        epoch_train_loss += train_loss\n",
    "        list_train_loss.append({'epoch':batch_idx/len(data_loader)+kwargs['epoch'], 'train_loss':train_loss})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt_iter += 1   \n",
    "        \n",
    "        if cnt_iter % args.save_every:\n",
    "            pass\n",
    "        print(train_loss)\n",
    "        \n",
    "    return models, list_train_loss\n",
    "\n",
    "\n",
    "def validate(models, data_loader, criterion, args):\n",
    "\n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    \n",
    "    mask_loss = nn.Cross_Entropy_Loss()\n",
    "    reg_loss = nn.MSE_Loss()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "        input_X = Variable(torch.from_numpy(input_X)).long()\n",
    "        ground_X = Variable(torch.from_numpy(ground_X)).long()\n",
    "        #ground_X = torch.from_numpy(ground_X).long()\n",
    "        A = Variable(torch.from_numpy(A))\n",
    "        logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "        \n",
    "        models['encoder'].eval()\n",
    "        if args.train_logp == True:\n",
    "            models[1].eval()\n",
    "        if args.train_mr == True:\n",
    "            models[2].eval()\n",
    "        if args.train_tpsa == True:\n",
    "            models[3].eval()\n",
    "        \n",
    "        X, A, molvec, ground_embed = models[0](input_X, A, ground_X, idx_M)\n",
    "        pred_mask = models[1](X, molvec, idx_M)\n",
    "        val_loss = mask_loss(pred_mask.float(),ground_embed.detach().long())\n",
    "        \n",
    "        \n",
    "        if args.train_logp == True:\n",
    "            pred_logp = models[2](molvec)\n",
    "            val_loss += reg_loss(pred_logp, logp)\n",
    "        if args.train_mr == True:\n",
    "            pred_mr = models[3](molvec)\n",
    "            val_loss += reg_loss(pred_mr, mr)\n",
    "        if args.train_tpsa == True:\n",
    "            pred_tpsa = modesl[4](molvec)\n",
    "            val_loss += reg_loss(pred_tpsa, tpsa)\n",
    "\n",
    "        pred_mask.require_grad = False\n",
    "        pred_logp.require_grad = False\n",
    "        pred_mr.require_grad = False\n",
    "        pred_tpsa.require_grad = False\n",
    "\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "\n",
    "def test(models, data_loader, args, **kwargs):\n",
    "    \n",
    "    list_logp, list_pred_logp = list(), list()\n",
    "    list_mr, list_pred_mr = list(), list()\n",
    "    list_tpsa, list_pred_tpsa = list(), list()\n",
    "    \n",
    "    logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    mask_loss = nn.Cross_Entropy_Loss()\n",
    "    reg_loss = nn.MSE_Loss()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "        input_X = Variable(torch.from_numpy(input_X)).long()\n",
    "        ground_X = Variable(torch.from_numpy(ground_X)).long()\n",
    "        #ground_X = torch.from_numpy(ground_X).long()\n",
    "        A = Variable(torch.from_numpy(A))\n",
    "        logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "        \n",
    "        models[0].eval()\n",
    "        if args.train_logp == True:\n",
    "            models[1].eval()\n",
    "        if args.train_mr == True:\n",
    "            models[2].eval()\n",
    "        if args.train_tpsa == True:\n",
    "            models[3].eval()\n",
    "        \n",
    "        X, A, molvec, ground_embed = models[0](input_X, A, ground_X, idx_M)\n",
    "        pred_mask = models[1](X, molvec, idx_M)\n",
    "        test_loss = mask_loss(pred_mask.float(),ground_embed.detach().long())\n",
    "        \n",
    "        if args.train_logp == True:\n",
    "            pred_logp = models[2](molvec)\n",
    "            test_loss += reg_loss(pred_logp, logp)\n",
    "            list_logp += logp.cpu().detach().numpy().tolist()\n",
    "            list_pred_logp += pred_logp.cpu().detach().numpy().tolist()\n",
    "            \n",
    "        if args.train_mr == True:\n",
    "            pred_mr = models[3](molvec)\n",
    "            test_loss += reg_loss(pred_mr, mr)\n",
    "            list_mr += logp.cpu().detach().numpy().tolist()\n",
    "            list_pred_mr += pred_logp.cpu().detach().numpy().tolist()\n",
    "            \n",
    "        if args.train_tpsa == True:\n",
    "            pred_tpsa = modesl[4](molvec)\n",
    "            test_loss += reg_loss(pred_tpsa, tpsa)\n",
    "            list_tpsa += logp.cpu().detach().numpy().tolist()\n",
    "            list_pred_tpsa += pred_logp.cpu().detach().numpy().tolist()\n",
    "        \n",
    "\n",
    "    if args.train_logp == True:\n",
    "        logp_mae = mean_absolute_error(list_logp, list_pred_logp)\n",
    "        logp_std = np.std(np.array(list_logp)-np.array(list_pred_logp))\n",
    "        \n",
    "    if args.train_mr == True:\n",
    "        mr_mae = mean_absolute_error(list_logp, list_pred_logp)\n",
    "        mr_std = np.std(np.array(list_logp)-np.array(list_pred_logp))\n",
    "        \n",
    "    if args.train_tpsa == True:\n",
    "        tpsa_mae = mean_absolute_error(list_tpsa, list_pred_tpsa)\n",
    "        tpsa_std = np.std(np.array(list_tpsa)-np.array(list_pred_tpsa))\n",
    "        \n",
    "    return (logp_mae, logp_std, list_logp, list_pred_logp, mr_mae, mr_std, list_mr, list_pred_mr, \n",
    "            tpsa_mae, tpsa_std, list_tpsa, list_pred_tpsa)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, model_name, epoch):\n",
    "    filename= '{}_{}_ckpt.pth'.format(model_name, epoch)\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "\n",
    "def experiment(data_loader, args):\n",
    "    ts = time.time()\n",
    "    encoder = Encoder(args)\n",
    "    classifier = Classifier(args.in_dim, args.out_dim, args.molvec_dim, args.vocab_size, args.dp_rate)\n",
    "    models = {'encoder':encoder, 'classifier':classifier}\n",
    "    if args.train_logp:\n",
    "        models['logP'] = Regressor(args.molvec_dim, args.dp_rate)\n",
    "    if args.train_mr:\n",
    "        models['mr'] = Regressor(args.molvec_dim, args.dp_rate)\n",
    "    if args.train_tpsa:\n",
    "        models['tpsa'] = Regressor(args.molvec_dim, args.dp_rate)\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "\n",
    "    trainable_parameters = list()\n",
    "    for key, model in models.items():\n",
    "        model.to(args.device)\n",
    "        trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        print(key, sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_logp_mae = list()\n",
    "    list_logp_std = list()\n",
    "    list_mr_mae = list()\n",
    "    list_mr_std = list()\n",
    "    list_tpsa_mae = list()\n",
    "    list_tpsa_std = list()\n",
    "    \n",
    "    tot_iter = 0\n",
    "    \n",
    "    args.best_mae = 10000\n",
    "    for epoch in range(args.epoch):\n",
    "        models, train_losses = train(models, data_loader, optimizer, args, **{'epoch':epoch})\n",
    "        val_loss = validate(models, data_loader, args)\n",
    "        \n",
    "        (logp_mae, logp_std, list_logp, list_pred_logp, mr_mae, mr_std, list_mr, list_pred_mr, \n",
    "         tpsa_mae, tpsa_std, list_tpsa, list_pred_tpsa)  = test(models, data_loader, args, **{'epoch':epoch})\n",
    "\n",
    "        list_train_loss += train_losses\n",
    "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "        list_logp_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "        list_logp_std.append({'epoch':epoch, 'std':logp_std})\n",
    "        list_mr_mae.append({'epoch':epoch, 'mae':logp_mae})\n",
    "        list_mr_std.append({'epoch':epoch, 'std':logp_std})\n",
    "        list_tpsa_mae.append({'epoch':epoch, 'mae':tpsa_mae})\n",
    "        list_tpsa_std.append({'epoch':epoch, 'std':tpsa_std})\n",
    "        \n",
    "        if args.best_mae > mae or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            #args.best_true_y = true_y\n",
    "            #args.best_pred_y = pred_y\n",
    "            \n",
    "        if total_iter % args.save_every == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'encoder': models['encoder'],\n",
    "                'encoder_state_dict': models['encoder'].state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                 })\n",
    "            \n",
    "        total_iter += 1\n",
    "             \n",
    "          \n",
    "    te = time.time()\n",
    "    \n",
    "    # Logging Experiment Results\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.logp_maes = list_logp_mae\n",
    "    args.logp_stds = list_logp_std\n",
    "    args.tpsa_maes = list_tpsa_mae\n",
    "    args.tpsa_stds = list_tpsa_std\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp0'\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = exp_name\n",
    "\n",
    "##### SIZE #####\n",
    "args.vocab_size = 41\n",
    "args.in_dim = 59\n",
    "args.out_dim = 512\n",
    "args.molvec_dim = 512\n",
    "\n",
    "\n",
    "##### MODEL #####\n",
    "args.num_layers = 2\n",
    "args.use_attn = True\n",
    "args.n_attn_heads = 8\n",
    "args.use_bn = True\n",
    "args.sc_type = 'sc'\n",
    "args.emb_train = True\n",
    "args.train_logp = True\n",
    "args.train_mr = True\n",
    "args.train_tpsa = True\n",
    "\n",
    "##### HYPERPARAMETERS #####\n",
    "args.optim = 'ADAM'\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.dp_rate = 0.1\n",
    "\n",
    "##### EXP #####\n",
    "args.epoch = 2\n",
    "args.batch_size = 1024\n",
    "args.test_batch_size = 1024\n",
    "args.save_every = 50\n",
    "\n",
    "##### DEVICE #####\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dir_files(dir_path):\n",
    "    list_file = [f for f in os.listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    return list_file\n",
    "\n",
    "train_dataset_path = './dataset/processed_zinc_smiles/data_xs/train'\n",
    "val_dataset_path = './dataset/processed_zinc_smiles/data_xs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = zincDataset(data_path=join(train_dataset_path, list_trains[0]))\n",
    "sampler = SequentialSampler(train_dataset)\n",
    "\n",
    "SortedBatchSampler = BatchSampler(sampler=sampler,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              collate_fn=postprocess_batch, \n",
    "                              num_workers=8, \n",
    "                              batch_sampler=SortedBatchSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = experiment(train_dataloader, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comet)",
   "language": "python",
   "name": "comet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
