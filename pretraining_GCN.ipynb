{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()   \n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "            \n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "            \n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2)) \n",
    "        attn_matrix = torch.mul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):    \n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gconv, Readout, BN1D, ResBlock, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, attn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.attn = attn\n",
    "        if self.attn is None:\n",
    "            self.fc = nn.Linear(input_dim, output_dim)\n",
    "            nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        if self.attn is None:\n",
    "            x = self.fc(X)\n",
    "            x = torch.matmul(A, x)\n",
    "        else:\n",
    "            x = self.attn(X, A)            \n",
    "        return x, A\n",
    "    \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = self.relu(torch.sum(molvec, dim=1))\n",
    "        return molvec\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x): \n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn, use_attn, dp_rate, sc_type, n_attn_head=None):\n",
    "        super(ResBlock, self).__init__()   \n",
    "        self.use_bn = use_bn\n",
    "        self.sc_type = sc_type\n",
    "        \n",
    "        attn = Attention(in_dim, out_dim, n_attn_head) if use_attn else None\n",
    "        self.gconv = GConv(in_dim, out_dim, attn)\n",
    "        \n",
    "        self.bn1 = BN1d(out_dim, use_bn)\n",
    "        self.dropout = nn.Dropout2d(p=dp_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if not self.sc_type in ['no', 'gsc', 'sc']:\n",
    "            raise Exception\n",
    "\n",
    "        if self.sc_type != 'no':\n",
    "            self.bn2 = BN1d(out_dim, use_bn)\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if in_dim != out_dim:\n",
    "                self.shortcut.add_module('shortcut', nn.Linear(in_dim, out_dim, bias=False))\n",
    "                \n",
    "        if self.sc_type == 'gsc':\n",
    "            self.g_fc1 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.g_fc2 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, A):     \n",
    "        x, A = self.gconv(X, A)\n",
    "\n",
    "        if self.sc_type == 'no': #no skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            return self.dropout(x), A\n",
    "        \n",
    "        elif self.sc_type == 'sc': # basic skip-connection\n",
    "            x = self.relu(self.bn1(x))\n",
    "            x = x + self.shortcut(X)          \n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "        elif self.sc_type == 'gsc': # gated skip-connection\n",
    "            x = self.relu(self.bn1(x)) \n",
    "            x1 = self.g_fc1(self.shortcut(X))\n",
    "            x2 = self.g_fc2(x)\n",
    "            gate_coef = self.sigmoid(x1+x2)\n",
    "            x = torch.mul(x1, gate_coef) + torch.mul(x2, 1.0-gate_coef)\n",
    "            return self.dropout(self.relu(self.bn2(x))), A\n",
    "        \n",
    "    \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bs = args.batch_size\n",
    "        self.molvec_dim = args.molvec_dim\n",
    "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train) \n",
    "        self.out_dim = args.out_dim\n",
    "        \n",
    "        self.gconvs = nn.ModuleList()\n",
    "        for i in range(args.num_layers):\n",
    "            if i==0:\n",
    "                self.gconvs.append(ResBlock(args.in_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "            else:\n",
    "                self.gconvs.append(ResBlock(self.out_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type, args.n_attn_heads))\n",
    "        self.readout = Readout(self.out_dim, self.molvec_dim)\n",
    "    \n",
    "    def forward(self, input_X, A):   \n",
    "        x, A, molvec = self.encoder(input_X, A)\n",
    "        return x, A, molvec\n",
    "     \n",
    "    def encoder(self, input_X, A):\n",
    "        x = self._embed(input_X)\n",
    "        for i, module in enumerate(self.gconvs):\n",
    "            x, A = module(x, A)\n",
    "        molvec = self.readout(x)\n",
    "        return x, A, molvec\n",
    "    \n",
    "    def _embed(self, x):\n",
    "        embed_x = self.embedding(x[:,:,0])\n",
    "        x = torch.cat((embed_x.float(), x[:,:,1:].float()), 2)\n",
    "        return x \n",
    "\n",
    "    def create_emb_layer(self, vocab_size, emb_train=False):\n",
    "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "        for i in range(vocab_size):\n",
    "            weight_matrix[i][i] = 1\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "\n",
    "        if not emb_train:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred_x, ground_x, vocab_size):\n",
    "    batch_size = ground_x.shape[0]\n",
    "    num_masking = ground_x.shape[1]\n",
    "    ground_x = ground_x.view(batch_size * num_masking, -1)\n",
    "    \n",
    "    symbol_loss = F.cross_entropy(pred_x[:,:vocab_size], ground_x[:, 0].detach())\n",
    "    degree_loss = F.cross_entropy(pred_x[:,vocab_size:vocab_size+6], ground_x[:,1:7].detach().max(dim=1)[1])\n",
    "    numH_loss = F.cross_entropy(pred_x[:,vocab_size+6:vocab_size+11], ground_x[:, 7:12].detach().max(dim=1)[1])\n",
    "    valence_loss = F.cross_entropy(pred_x[:,vocab_size+11:vocab_size+17], ground_x[:,12:18].detach().max(dim=1)[1])\n",
    "    isarom_loss = F.binary_cross_entropy(torch.sigmoid(pred_x[:,-1]), ground_x[:,-1].detach().float())\n",
    "    total_loss = symbol_loss + degree_loss + numH_loss + valence_loss + isarom_loss\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier & Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, molvec_dim, vocab_size, dropout_rate=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.vs = vocab_size\n",
    "    \n",
    "        self.fc1 = nn.Linear(self.molvec_dim + self.out_dim, args.in_dim)\n",
    "        self.fc2 = nn.Linear(self.in_dim, args.in_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.param_initializer()\n",
    "        \n",
    "    def forward(self, X, molvec, idx_M):\n",
    "        batch_size = X.shape[0]\n",
    "        #print('idx_M', idx_M.shape)\n",
    "        num_masking = idx_M.shape[1]\n",
    "        probs_atom = list()\n",
    "        probs_degree = list()\n",
    "        probs_numH = list()\n",
    "        probs_valence = list()\n",
    "        probs_isarom = list()\n",
    "        \n",
    "        molvec = torch.unsqueeze(molvec, 1)\n",
    "        molvec = molvec.expand(batch_size, num_masking, molvec.shape[-1])\n",
    "        \n",
    "        list_concat_x = list()\n",
    "        for i in range(batch_size):\n",
    "            target_x = torch.index_select(X[i], 0, idx_M[i])\n",
    "            concat_x = torch.cat((target_x, molvec[i]), dim=1)\n",
    "            list_concat_x.append(concat_x)\n",
    "            \n",
    "        concat_x = torch.stack(list_concat_x)\n",
    "        pred_x = self.classify(concat_x)\n",
    "        pred_x = pred_x.view(batch_size * num_masking, -1)\n",
    "        return pred_x\n",
    "    \n",
    "    def classify(self, concat_x):\n",
    "        x = self.relu(self.fc1(concat_x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "    \n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, molvec_dim, dropout_rate):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.reg_fc1 = nn.Linear(self.molvec_dim, self.molvec_dim//2)\n",
    "        self.reg_fc2 = nn.Linear(self.molvec_dim//2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, molvec):\n",
    "        x = self.relu(self.reg_fc1(molvec))\n",
    "        x = self.reg_fc2(x)\n",
    "        return torch.squeeze(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, optimizer, dataloader, epoch, cnt_iter, args):\n",
    "    t = time.time()\n",
    "    list_train_loss = list()\n",
    "    epoch = epoch\n",
    "    cnt_iter = cnt_iter\n",
    "    reg_loss = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epoch, args.epoch+1):\n",
    "        epoch_train_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader['train']):\n",
    "            \n",
    "            # Setting Train Mode\n",
    "            for _, model in models.items():\n",
    "                model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get Batch Sample from DataLoader\n",
    "            input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "            input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "            A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "            mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "            logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "            ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "            idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "\n",
    "            # Encoding Molecule\n",
    "            X, A, molvec = models['encoder'](input_X, A)\n",
    "            pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "\n",
    "            # Compute Mask Task Loss & Property Regression Loss\n",
    "            mask_loss = compute_loss(pred_mask, ground_X, args.vocab_size)\n",
    "            loss = mask_loss\n",
    "\n",
    "            if args.train_logp:\n",
    "                pred_logp = models['logP'](molvec)\n",
    "                logP_loss = reg_loss(pred_logp, logp)\n",
    "                loss += logP_loss\n",
    "                train_writer.add_scalar('auxilary/logP', logP_loss, cnt_iter)\n",
    "            if args.train_mr:\n",
    "                pred_mr = models['mr'](molvec)\n",
    "                mr_loss = reg_loss(pred_mr, mr)\n",
    "                loss += mr_loss\n",
    "                train_writer.add_scalar('auxilary/mr', mr_loss, cnt_iter)\n",
    "\n",
    "            if args.train_tpsa:\n",
    "                pred_tpsa = models['tpsa'](molvec)\n",
    "                tpsa_loss = reg_loss(pred_tpsa, tpsa)\n",
    "                loss += tpsa_loss\n",
    "                train_writer.add_scalar('auxilary/tpsa', tpsa_loss, cnt_iter)\n",
    "\n",
    "            train_writer.add_scalar('loss/total', loss, cnt_iter)\n",
    "            train_writer.add_scalar('loss/mask', mask_loss, cnt_iter)\n",
    "\n",
    "\n",
    "            epoch_train_loss += loss / len(batch)\n",
    "            list_train_loss.append({'epoch':batch_idx/len(dataloader['train'])+epoch, 'train_loss':loss})\n",
    "            \n",
    "            # Backprogating and Updating Parameter\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cnt_iter += 1   \n",
    "\n",
    "            # Save Model \n",
    "            if cnt_iter % args.save_every == 0:\n",
    "                filename = save_checkpoint(epoch, cnt_iter, models, optimizer, args)\n",
    "                logger.info('Saved Model as {}'.format(filename))\n",
    "\n",
    "            # Validate Model\n",
    "            if cnt_iter % args.validate_every == 0:\n",
    "                optimizer.zero_grad()\n",
    "                validate(models, dataloader['val'], args, cnt_iter=cnt_iter, epoch=epoch)\n",
    "                t = time.time()\n",
    "\n",
    "            # Prompting Status\n",
    "            if cnt_iter % args.log_every == 0:\n",
    "                output = \"[T] E:{:3}. P:{:>2.1f}%. Loss:{:>9.3}. Mask Loss:{:>9.3}. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "                elapsed = time.time() - t\n",
    "                process_speed = (args.batch_size * args.log_every) / elapsed\n",
    "                output = output.format(epoch, batch_idx / len(dataloader['train']) * 100.0, loss, mask_loss, process_speed, cnt_iter, elapsed,)\n",
    "                t = time.time()\n",
    "                logger.info(output)\n",
    "\n",
    "    return models, list_train_loss\n",
    "\n",
    "\n",
    "def experiment(dataloader, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    # Construct Model\n",
    "    encoder = Encoder(args)\n",
    "    classifier = Classifier(args.in_dim, args.out_dim, args.molvec_dim, args.vocab_size, args.dp_rate)\n",
    "    models = {'encoder': encoder, 'classifier': classifier}\n",
    "    if args.train_logp:\n",
    "        models.update({'logP': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_mr:\n",
    "        models.update({'mr': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "    if args.train_tpsa:\n",
    "        models.update({'tpsa': Regressor(args.molvec_dim, args.dp_rate)})\n",
    "        \n",
    "    # Initialize Optimizer\n",
    "    logger.info('####### Model Constructed #######')\n",
    "    trainable_parameters = list()\n",
    "    for key, model in models.items():\n",
    "        model.to(args.device)\n",
    "        trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        logger.info('{:10}: {:>10} parameters'.format(key, sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    logger.info('#################################')\n",
    "    \n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Reload Checkpoint Model\n",
    "    epoch = 0\n",
    "    cnt_iter = 0\n",
    "    if args.ck_filename:\n",
    "        epoch, cnt_iter, models, optimizer = load_checkpoint(models, optimizer, args.ck_filename, args)\n",
    "        logger.info('Loaded Model from {}'.format(args.ck_filename))\n",
    "\n",
    "    # Initialize Data Logger\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_logp_mae = list()\n",
    "    list_logp_std = list()\n",
    "    list_mr_mae = list()\n",
    "    list_mr_std = list()\n",
    "    list_tpsa_mae = list()\n",
    "    list_tpsa_std = list()\n",
    "    \n",
    "    # Train Model\n",
    "    train(models, optimizer, dataloader, epoch, cnt_iter, args)\n",
    "\n",
    "    # Logging Experiment Result\n",
    "    te = time.time()    \n",
    "    args.elapsed = te-ts\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(models, data_loader, args, **kwargs):\n",
    "\n",
    "    t = time.time()\n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = kwargs['cnt_iter']\n",
    "    epoch = kwargs['epoch']\n",
    "    temp_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "    \n",
    "    mask_loss = []\n",
    "    logP_loss = []\n",
    "    mr_loss = []\n",
    "    tpsa_loss = []\n",
    "    \n",
    "    list_logp, list_pred_logp = [], []\n",
    "    list_mr, list_pred_mr = [], []\n",
    "    list_tpsa, list_pred_tpsa = [], []\n",
    "    logp_mae, logp_std, mr_mae, mr_std, tpsa_mae, tpsa_std = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Initialization Model with Evaluation Mode\n",
    "    for _, model in models.items():\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_X, A, mol_prop, ground_X, idx_M = batch\n",
    "            input_X = Variable(torch.from_numpy(input_X)).to(args.device).long()\n",
    "            A = Variable(torch.from_numpy(A)).to(args.device).float()\n",
    "            mol_prop = Variable(torch.from_numpy(mol_prop)).to(args.device).float()\n",
    "            logp, mr, tpsa = mol_prop[:,0], mol_prop[:,1], mol_prop[:,2]\n",
    "            ground_X = Variable(torch.from_numpy(ground_X)).to(args.device).long()\n",
    "            idx_M = Variable(torch.from_numpy(idx_M)).to(args.device).long()\n",
    "\n",
    "            # Encoding Molecule\n",
    "            X, A, molvec = models['encoder'](input_X, A)\n",
    "            pred_mask = models['classifier'](X, molvec, idx_M)\n",
    "            \n",
    "            # Compute Mask Task Loss & Property Regression Loss\n",
    "            mask_loss.append(compute_loss(pred_mask, ground_X, args.vocab_size).item())\n",
    "\n",
    "            if args.train_logp:\n",
    "                pred_logp = models['logP'](molvec)\n",
    "                logP_loss.append(reg_loss(pred_logp, logp).item())\n",
    "            if args.train_mr:\n",
    "                pred_mr = models['mr'](molvec)\n",
    "                mr_loss.append(reg_loss(pred_mr, mr).item())\n",
    "            if args.train_tpsa:\n",
    "                pred_tpsa = models['tpsa'](molvec)\n",
    "                tpsa_loss.append(reg_loss(pred_tpsa, tpsa).item())\n",
    "\n",
    "            temp_iter += 1   \n",
    "\n",
    "            # Prompting Status\n",
    "            if temp_iter % (args.log_every * 4) == 0:\n",
    "                output = \"[V] E:{:3}. P:{:>2.1f}%. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "                elapsed = time.time() - t\n",
    "                process_speed = (args.test_batch_size * args.log_every) / elapsed\n",
    "                output = output.format(epoch, batch_idx / len(data_loader) * 100.0, process_speed, temp_iter, elapsed,)\n",
    "                t = time.time()\n",
    "                logger.info(output)\n",
    "                \n",
    "    mask_loss = np.mean(np.array(mask_loss))\n",
    "    loss = mask_loss\n",
    "    if args.train_logp:\n",
    "        logP_loss = np.mean(np.array(logP_loss))\n",
    "        loss += logP_loss\n",
    "        val_writer.add_scalar('auxilary/logP', logP_loss, cnt_iter)\n",
    "    if args.train_mr:\n",
    "        mr_loss = np.mean(np.array(mr_loss))\n",
    "        loss += mr_loss\n",
    "        val_writer.add_scalar('auxilary/mr', mr_loss, cnt_iter)\n",
    "\n",
    "    if args.train_tpsa:\n",
    "        tpsa_loss = np.mean(np.array(tpsa_loss))\n",
    "        loss += tpsa_loss\n",
    "        val_writer.add_scalar('auxilary/tpsa', tpsa_loss, cnt_iter)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate overall MAE and STD value      \n",
    "    if args.train_logp:\n",
    "        logp_mae = mean_absolute_error(list_logp, list_pred_logp)\n",
    "        logp_std = np.std(np.array(list_logp)-np.array(list_pred_logp))\n",
    "        \n",
    "    if args.train_mr:\n",
    "        mr_mae = mean_absolute_error(list_mr, list_pred_mr)\n",
    "        mr_std = np.std(np.array(list_mr)-np.array(list_pred_mr))\n",
    "        \n",
    "    if args.train_tpsa:\n",
    "        tpsa_mae = mean_absolute_error(list_tpsa, list_pred_tpsa)\n",
    "        tpsa_std = np.std(np.array(list_tpsa)-np.array(list_pred_tpsa))\n",
    "    \"\"\"\n",
    "        \n",
    "    val_writer.add_scalar('loss/total', loss, cnt_iter)\n",
    "    val_writer.add_scalar('loss/mask', mask_loss, cnt_iter)\n",
    "\n",
    "    output = \"[V] E:{:3}. P:{:>2.1f}%. Loss:{:>9.3}. Mask Loss:{:>9.3}. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "    elapsed = time.time() - t\n",
    "    process_speed = (args.test_batch_size * args.log_every) / elapsed\n",
    "    output = output.format(epoch, batch_idx / len(data_loader) * 100.0, loss, mask_loss, process_speed, cnt_iter, elapsed,)\n",
    "    t = time.time()\n",
    "    logger.info(output)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return epoch_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "##### SIZE #####\n",
    "args.vocab_size = 41\n",
    "args.in_dim = 59\n",
    "args.out_dim = 256\n",
    "args.molvec_dim = 512\n",
    "\n",
    "##### MODEL #####\n",
    "args.num_layers = 6\n",
    "args.use_attn = True\n",
    "args.n_attn_heads = 8\n",
    "args.use_bn = True\n",
    "args.sc_type = 'sc'\n",
    "args.emb_train = True\n",
    "args.train_logp = True\n",
    "args.train_mr = True\n",
    "args.train_tpsa = True\n",
    "\n",
    "##### HYPERPARAMETERS #####\n",
    "args.optim = 'ADAM'\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.dp_rate = 0.1\n",
    "\n",
    "##### EXP #####\n",
    "args.epoch = 100\n",
    "args.batch_size = 512\n",
    "args.test_batch_size = 512\n",
    "args.save_every = 100\n",
    "args.validate_every = 100\n",
    "args.log_every = 20\n",
    "\n",
    "##### DEVICE #####\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "##### LOGGING #####\n",
    "args.log_path = 'runs'\n",
    "args.model_name = 'exp_test3'\n",
    "args.model_explain = make_model_comment(args)\n",
    "train_writer = SummaryWriter(join(args.log_path, args.model_name+'_train'))\n",
    "val_writer = SummaryWriter(join(args.log_path, args.model_name+'_val'))\n",
    "train_writer.add_text(tag='model', text_string='{}:{}'.format(args.model_name, args.model_explain), global_step= 0)\n",
    "logger = get_logger(join(args.log_path, args.model_name+'_train'))\n",
    "\n",
    "##### RESUME TRAINING #####\n",
    "args.ck_filename = None # Example: 'model_ck_000_000000100.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/processed_zinc_smiles/data_xs/train'\n",
    "val_dataset_path = './dataset/processed_zinc_smiles/data_xs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)\n",
    "list_vals = get_dir_files(val_dataset_path)\n",
    "\n",
    "train_dataloader = zincDataLoader(join(train_dataset_path, list_trains[0]),\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=True,\n",
    "                                  num_workers=8)\n",
    "\n",
    "val_dataloader = zincDataLoader(join(val_dataset_path, list_vals[0]),\n",
    "                                  batch_size=args.test_batch_size,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=False,\n",
    "                                  num_workers=8)\n",
    "\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:11:42,018 [INFO] ####### Model Constructed #######\n",
      "2018-12-04 02:11:42,018 [INFO] ####### Model Constructed #######\n",
      "2018-12-04 02:11:42,028 [INFO] encoder   :     547985 parameters\n",
      "2018-12-04 02:11:42,028 [INFO] encoder   :     547985 parameters\n",
      "2018-12-04 02:11:42,030 [INFO] classifier:      48911 parameters\n",
      "2018-12-04 02:11:42,030 [INFO] classifier:      48911 parameters\n",
      "2018-12-04 02:11:42,032 [INFO] logP      :     131585 parameters\n",
      "2018-12-04 02:11:42,032 [INFO] logP      :     131585 parameters\n",
      "2018-12-04 02:11:42,034 [INFO] mr        :     131585 parameters\n",
      "2018-12-04 02:11:42,034 [INFO] mr        :     131585 parameters\n",
      "2018-12-04 02:11:42,035 [INFO] tpsa      :     131585 parameters\n",
      "2018-12-04 02:11:42,035 [INFO] tpsa      :     131585 parameters\n",
      "2018-12-04 02:11:42,036 [INFO] #################################\n",
      "2018-12-04 02:11:42,036 [INFO] #################################\n",
      "2018-12-04 02:11:48,356 [INFO] [T] E:  0. P:2.3%. Loss:     10.7. Mask Loss:     10.7. 1620.8 mol/sec. Iter:    20.  Elapsed:   6.3 sec.\n",
      "2018-12-04 02:11:48,356 [INFO] [T] E:  0. P:2.3%. Loss:     10.7. Mask Loss:     10.7. 1620.8 mol/sec. Iter:    20.  Elapsed:   6.3 sec.\n",
      "2018-12-04 02:11:53,104 [INFO] [T] E:  0. P:4.7%. Loss:     7.76. Mask Loss:     7.76. 2156.5 mol/sec. Iter:    40.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:11:53,104 [INFO] [T] E:  0. P:4.7%. Loss:     7.76. Mask Loss:     7.76. 2156.5 mol/sec. Iter:    40.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:11:57,855 [INFO] [T] E:  0. P:7.1%. Loss:     7.63. Mask Loss:     7.63. 2155.5 mol/sec. Iter:    60.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:11:57,855 [INFO] [T] E:  0. P:7.1%. Loss:     7.63. Mask Loss:     7.63. 2155.5 mol/sec. Iter:    60.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:02,694 [INFO] [T] E:  0. P:9.5%. Loss:     8.68. Mask Loss:     8.68. 2116.0 mol/sec. Iter:    80.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:02,694 [INFO] [T] E:  0. P:9.5%. Loss:     8.68. Mask Loss:     8.68. 2116.0 mol/sec. Iter:    80.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:07,390 [INFO] Saved Model as model_ck_000_000000100.tar\n",
      "2018-12-04 02:12:07,390 [INFO] Saved Model as model_ck_000_000000100.tar\n",
      "2018-12-04 02:12:15,233 [INFO] [V] E:  0. P:37.6%. 1306.5 mol/sec. Iter:    80.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:15,233 [INFO] [V] E:  0. P:37.6%. 1306.5 mol/sec. Iter:    80.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:23,041 [INFO] [V] E:  0. P:75.7%. 1311.4 mol/sec. Iter:   160.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:23,041 [INFO] [V] E:  0. P:75.7%. 1311.4 mol/sec. Iter:   160.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:28,480 [INFO] [V] E:  0. P:99.5%. Loss:     7.45. Mask Loss:     5.22. 1883.0 mol/sec. Iter:   100.  Elapsed:   5.4 sec.\n",
      "2018-12-04 02:12:28,480 [INFO] [V] E:  0. P:99.5%. Loss:     7.45. Mask Loss:     5.22. 1883.0 mol/sec. Iter:   100.  Elapsed:   5.4 sec.\n",
      "2018-12-04 02:12:28,663 [INFO] [T] E:  0. P:11.9%. Loss:     6.75. Mask Loss:     6.75. 1533916891.4 mol/sec. Iter:   100.  Elapsed:   0.0 sec.\n",
      "2018-12-04 02:12:28,663 [INFO] [T] E:  0. P:11.9%. Loss:     6.75. Mask Loss:     6.75. 1533916891.4 mol/sec. Iter:   100.  Elapsed:   0.0 sec.\n",
      "2018-12-04 02:12:33,602 [INFO] [T] E:  0. P:14.4%. Loss:     6.58. Mask Loss:     6.58. 2073.2 mol/sec. Iter:   120.  Elapsed:   4.9 sec.\n",
      "2018-12-04 02:12:33,602 [INFO] [T] E:  0. P:14.4%. Loss:     6.58. Mask Loss:     6.58. 2073.2 mol/sec. Iter:   120.  Elapsed:   4.9 sec.\n",
      "2018-12-04 02:12:38,583 [INFO] [T] E:  0. P:16.8%. Loss:     7.35. Mask Loss:     7.35. 2055.7 mol/sec. Iter:   140.  Elapsed:   5.0 sec.\n",
      "2018-12-04 02:12:38,583 [INFO] [T] E:  0. P:16.8%. Loss:     7.35. Mask Loss:     7.35. 2055.7 mol/sec. Iter:   140.  Elapsed:   5.0 sec.\n",
      "2018-12-04 02:12:43,326 [INFO] [T] E:  0. P:19.2%. Loss:     7.13. Mask Loss:     7.13. 2159.3 mol/sec. Iter:   160.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:43,326 [INFO] [T] E:  0. P:19.2%. Loss:     7.13. Mask Loss:     7.13. 2159.3 mol/sec. Iter:   160.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:48,050 [INFO] [T] E:  0. P:21.6%. Loss:     6.94. Mask Loss:     6.94. 2167.7 mol/sec. Iter:   180.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:48,050 [INFO] [T] E:  0. P:21.6%. Loss:     6.94. Mask Loss:     6.94. 2167.7 mol/sec. Iter:   180.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:52,927 [INFO] Saved Model as model_ck_000_000000200.tar\n",
      "2018-12-04 02:12:52,927 [INFO] Saved Model as model_ck_000_000000200.tar\n",
      "2018-12-04 02:13:00,414 [INFO] [V] E:  0. P:37.6%. 1368.4 mol/sec. Iter:    80.  Elapsed:   7.5 sec.\n",
      "2018-12-04 02:13:00,414 [INFO] [V] E:  0. P:37.6%. 1368.4 mol/sec. Iter:    80.  Elapsed:   7.5 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-545d40d4a4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-864c9820963d>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dataloader, args)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Logging Experiment Result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-864c9820963d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, optimizer, dataloader, epoch, cnt_iter, args)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnt_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-093ef6fd4565>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(models, data_loader, args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0minput_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0minput_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = experiment(dataloader, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comet)",
   "language": "python",
   "name": "comet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
