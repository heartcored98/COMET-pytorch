{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "from metric import *\n",
    "\n",
    "%matplotlib inline\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self, step):\n",
    "        \"Update parameters and rate\"\n",
    "        rate = self.rate(step)\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step = None):\n",
    "        step += 1\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()   \n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "            \n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "            \n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2)) \n",
    "        attn_matrix = torch.mul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "    \n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):    \n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gconv, Readout, BN1D, ResBlock, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "\n",
    "    \"\"\" Ref: https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py\n",
    "        Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu}\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_attn_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_attn_heads = num_attn_head\n",
    "        self.attn_dim = output_dim // num_attn_head\n",
    "        self.projection = nn.ModuleList([nn.Linear(input_dim, self.attn_dim) for i in range(self.num_attn_heads)])\n",
    "        self.coef_matrix = nn.ParameterList([nn.Parameter(torch.FloatTensor(self.attn_dim, self.attn_dim)) for i in range(self.num_attn_heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        list_X_head = list()\n",
    "        for i in range(self.num_attn_heads):\n",
    "            X_projected = self.projection[i](X)\n",
    "            attn_matrix = self.attn_coeff(X_projected, A, self.coef_matrix[i])\n",
    "            X_head = torch.matmul(attn_matrix, X_projected)\n",
    "            list_X_head.append(X_head)\n",
    "\n",
    "        X = torch.cat(list_X_head, dim=2)\n",
    "        X = self.relu(X)\n",
    "        return X\n",
    "\n",
    "    def attn_coeff(self, X_projected, A, C):\n",
    "        X = torch.einsum('akj,ij->aki', (X_projected, C))\n",
    "        attn_matrix = torch.matmul(X, torch.transpose(X_projected, 1, 2))\n",
    "        attn_matrix = torch.mul(A, attn_matrix)\n",
    "        attn_matrix = self.dropout(self.tanh(attn_matrix))\n",
    "        return attn_matrix\n",
    "\n",
    "    def param_initializer(self):\n",
    "        for i in range(self.num_attn_heads):\n",
    "            nn.init.xavier_normal_(self.projection[i].weight.data)\n",
    "            nn.init.xavier_normal_(self.coef_matrix[i].data)\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# ===== Gconv, Readout, BN1D, ResBlock, Encoder =====#\n",
    "#####################################################\n",
    "\n",
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, attn, act=ACT2FN['relu']):\n",
    "        super(GConv, self).__init__()\n",
    "        self.attn = attn\n",
    "        if self.attn is None:\n",
    "            self.fc = nn.Linear(input_dim, output_dim)\n",
    "            self.act = act\n",
    "            nn.init.xavier_normal_(self.fc.weight.data)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        if self.attn is None:\n",
    "            x = self.act(self.fc(X))\n",
    "            x = torch.matmul(A, x)\n",
    "        else:\n",
    "            x = self.attn(X, A)\n",
    "        return x, A\n",
    "\n",
    "\n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
    "\n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = torch.mean(molvec, dim=1)\n",
    "        return molvec\n",
    "\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn=True):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn, use_attn, dp_rate, sc_type, n_attn_head=None, act=ACT2FN['relu']):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.sc_type = sc_type\n",
    "\n",
    "        attn = Attention(in_dim, out_dim, n_attn_head) if use_attn else None\n",
    "        self.gconv = GConv(in_dim, out_dim, attn)\n",
    "\n",
    "        self.bn1 = BN1d(out_dim, use_bn)\n",
    "        self.dropout = nn.Dropout2d(p=dp_rate)\n",
    "        self.act = act\n",
    "\n",
    "        if not self.sc_type in ['no', 'gsc', 'sc']:\n",
    "            raise Exception\n",
    "\n",
    "        if self.sc_type != 'no':\n",
    "            self.bn2 = BN1d(out_dim, use_bn)\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if in_dim != out_dim:\n",
    "                self.shortcut.add_module('shortcut', nn.Linear(in_dim, out_dim, bias=False))\n",
    "\n",
    "        if self.sc_type == 'gsc':\n",
    "            self.g_fc1 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.g_fc2 = nn.Linear(out_dim, out_dim, bias=True)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        x, A = self.gconv(X, A)\n",
    "\n",
    "        if self.sc_type == 'no':  # no skip-connection\n",
    "            x = self.act(self.bn1(x))\n",
    "            return self.dropout(x), A\n",
    "\n",
    "        elif self.sc_type == 'sc': # basic skip-connection\n",
    "            x = self.act(self.bn1(x))\n",
    "            x = x + self.shortcut(X)\n",
    "            return self.dropout(self.act(self.bn2(x))), A\n",
    "\n",
    "        elif self.sc_type == 'gsc': # gated skip-connection\n",
    "            x = self.act(self.bn1(x))\n",
    "            x1 = self.g_fc1(self.shortcut(X))\n",
    "            x2 = self.g_fc2(x)\n",
    "            gate_coef = self.sigmoid(x1 +x2)\n",
    "            x = torch.mul(x1, gate_coef) + torch.mul(x2, 1.0-gate_coef)\n",
    "            return self.dropout(self.act(self.bn2(x))), A\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bs = args.batch_size\n",
    "        self.molvec_dim = args.molvec_dim\n",
    "        self.embedding = self.create_emb_layer([args.vocab_size, args.degree_size,\n",
    "                                                args.numH_size, args.valence_size,\n",
    "                                                args.isarom_size],  args.emb_train)\n",
    "        self.out_dim = args.out_dim\n",
    "\n",
    "        # Graph Convolution Layers with Readout Layer\n",
    "        self.gconvs = nn.ModuleList()\n",
    "        for i in range(args.n_layer):\n",
    "            if i== 0:\n",
    "                self.gconvs.append(\n",
    "                    ResBlock(args.in_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type,\n",
    "                             args.n_attn_heads, ACT2FN[args.act]))\n",
    "            else:\n",
    "                self.gconvs.append(\n",
    "                    ResBlock(self.out_dim, self.out_dim, args.use_bn, args.use_attn, args.dp_rate, args.sc_type,\n",
    "                             args.n_attn_heads, ACT2FN[args.act]))\n",
    "        self.readout = Readout(self.out_dim, self.molvec_dim)\n",
    "\n",
    "        # Molecular Vector Transformation\n",
    "        self.fc1 = nn.Linear(self.molvec_dim, self.molvec_dim)\n",
    "        self.fc2 = nn.Linear(self.molvec_dim, self.molvec_dim)\n",
    "        self.fc3 = nn.Linear(self.molvec_dim, self.molvec_dim)\n",
    "        self.bn1 = BN1d(self.molvec_dim)\n",
    "        self.bn2 = BN1d(self.molvec_dim)\n",
    "        self.act = ACT2FN[args.act]\n",
    "        self.dropout = nn.Dropout(p=args.dp_rate)\n",
    "\n",
    "\n",
    "    def forward(self, input_X, A):\n",
    "        x, A, molvec = self.encoder(input_X, A)\n",
    "        molvec = self.dropout(self.bn1(self.act(self.fc1(molvec))))\n",
    "        molvec = self.dropout(self.bn2(self.act(self.fc2(molvec))))\n",
    "        molvec = self.fc3(molvec)\n",
    "        return x, A, molvec\n",
    "\n",
    "    def encoder(self, input_X, A):\n",
    "        x = self._embed(input_X)\n",
    "        for i, module in enumerate(self.gconvs):\n",
    "            x, A = module(x, A)\n",
    "        molvec = self.readout(x)\n",
    "        return x, A, molvec\n",
    "\n",
    "    def _embed(self, x):\n",
    "        list_embed = list()\n",
    "        for i in range(5):\n",
    "            list_embed.append(self.embedding[i](x[:, :, i].long()))\n",
    "        x = torch.cat(list_embed, 2)\n",
    "        return x\n",
    "\n",
    "    def create_emb_layer(self, list_vocab_size, emb_train=False):\n",
    "        list_emb_layer = nn.ModuleList()\n",
    "        for i, vocab_size in enumerate(list_vocab_size):\n",
    "            vocab_size += 1\n",
    "            emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "            weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "            for i in range(vocab_size):\n",
    "                weight_matrix[i][i] = 1\n",
    "            emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "            emb_layer.weight.requires_grad = emb_train\n",
    "            list_emb_layer.append(emb_layer)\n",
    "        return list_emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_loss(pred_x, ground_x, vocab_size):\n",
    "    symbol_loss = F.cross_entropy(pred_x[:,:vocab_size], ground_x[:,0].detach().long())\n",
    "    degree_loss = F.cross_entropy(pred_x[:,vocab_size:vocab_size+6], ground_x[:,1].detach().long())\n",
    "    numH_loss = F.cross_entropy(pred_x[:,vocab_size+6:vocab_size+11], ground_x[:,2].detach().long())\n",
    "    valence_loss = F.cross_entropy(pred_x[:,vocab_size+11:vocab_size+17], ground_x[:,3].detach().long())\n",
    "    isarom_loss = F.binary_cross_entropy(torch.sigmoid(pred_x[:,-2]), ground_x[:,4].detach().float())\n",
    "    partial_loss = F.mse_loss(pred_x[:,-1], ground_x[:,5])\n",
    "    total_loss = symbol_loss + degree_loss + numH_loss + valence_loss + isarom_loss + partial_loss\n",
    "    return symbol_loss, degree_loss, numH_loss, valence_loss, isarom_loss, partial_loss, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier & Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim, classifier_dim, in_dim, dropout_rate=0.3, act=ACT2FN['relu']):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.classifier_dim = classifier_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.molvec_dim + self.out_dim, self.classifier_dim)\n",
    "        self.fc2 = nn.Linear(self.classifier_dim, self.classifier_dim // 2)\n",
    "        self.fc3 = nn.Linear(self.classifier_dim // 2, self.in_dim)\n",
    "        self.bn1 = BN1d(self.classifier_dim)\n",
    "        self.bn2 = BN1d(self.classifier_dim // 2)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, X, molvec, idx_M):\n",
    "        batch_size = X.shape[0]\n",
    "        num_masking = idx_M.shape[1]\n",
    "\n",
    "        molvec = torch.unsqueeze(molvec, 1)\n",
    "        molvec = molvec.expand(batch_size, num_masking, molvec.shape[-1])\n",
    "\n",
    "        list_concat_x = list()\n",
    "        for i in range(batch_size):\n",
    "            target_x = torch.index_select(X[i], 0, idx_M[i])\n",
    "            concat_x = torch.cat((target_x, molvec[i]), dim=1)\n",
    "            list_concat_x.append(concat_x)\n",
    "\n",
    "        concat_x = torch.stack(list_concat_x)\n",
    "        pred_x = self.classify(concat_x)\n",
    "        pred_x = pred_x.view(batch_size * num_masking, -1)\n",
    "        return pred_x\n",
    "\n",
    "    def classify(self, concat_x):\n",
    "        x = self.dropout(self.bn1(self.act(self.fc1(concat_x))))\n",
    "        x = self.dropout(self.bn2(self.act(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, molvec_dim, classifier_dim, num_aux_task=5, dropout_rate=0.3, act=ACT2FN['relu']):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.molvec_dim = molvec_dim\n",
    "        self.classifier_dim = classifier_dim\n",
    "        self.fc1 = nn.Linear(self.molvec_dim, self.classifier_dim)\n",
    "        self.fc2 = nn.Linear(self.classifier_dim, self.classifier_dim // 2)\n",
    "        self.fc3 = nn.Linear(self.classifier_dim // 2, num_aux_task)\n",
    "        self.bn1 = nn.BatchNorm1d(self.classifier_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(self.classifier_dim // 2)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.act = act\n",
    "        self.param_initializer()\n",
    "\n",
    "    def forward(self, molvec):\n",
    "        x = self.dropout(self.bn1(self.act(self.fc1(molvec))))\n",
    "        x = self.dropout(self.bn2(self.act(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def param_initializer(self):\n",
    "        nn.init.xavier_normal_(self.fc1.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc2.weight.data)\n",
    "        nn.init.xavier_normal_(self.fc3.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, optimizer, dataloader, epoch, cnt_iter, args):\n",
    "    t = time.time()\n",
    "    list_train_loss = list()\n",
    "    epoch = epoch\n",
    "    cnt_iter = cnt_iter\n",
    "    reg_loss = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epoch, args.epoch+1):\n",
    "        for batch_idx, batch in enumerate(dataloader['train']):\n",
    "            t1 = time.time()\n",
    "            # Setting Train Mode\n",
    "            for _, model in models.items():\n",
    "                model.train()\n",
    "\n",
    "            optimizer['mask'].zero_grad()\n",
    "            optimizer['auxiliary'].zero_grad()\n",
    "\n",
    "            # Get Batch Sample from DataLoader\n",
    "            predict_idx, X, mask_X, true_X, A, C = batch\n",
    "\n",
    "            # Normalize A matrix in order to prevent overflow\n",
    "\n",
    "            # Convert Tensor into Variable and Move to CUDA\n",
    "            mask_idx = Variable(predict_idx).to(args.device).long()\n",
    "            input_X = Variable(X).to(args.device).float()\n",
    "            mask_X = Variable(mask_X).to(args.device).float()\n",
    "            true_X = Variable(true_X).to(args.device).float()\n",
    "            input_A = Variable(A).to(args.device).float()\n",
    "            #     mask_A = Variable(mask_A).to(args.device).float()\n",
    "            input_C = Variable(C).to(args.device).float()\n",
    "\n",
    "            t2 = time.time()\n",
    "            # Encoding Masked Molecule\n",
    "            encoded_X, _, molvec = models['encoder'](mask_X, input_A)\n",
    "            pred_X = models['classifier'](encoded_X, molvec, mask_idx)\n",
    "\n",
    "            # Compute Mask Task Loss\n",
    "            symbol_loss, degree_loss, numH_loss, valence_loss, isarom_loss, partial_loss, mask_loss = Compute_loss(pred_X, true_X, args.vocab_size)\n",
    "\n",
    "            # Backprogating and Updating Parameter\n",
    "            mask_loss.backward()\n",
    "            optimizer['mask'].step(cnt_iter)\n",
    "            train_writer.add_scalar('1.status/lr', optimizer['mask'].rate(cnt_iter), cnt_iter)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            t3 = time.time()\n",
    "\n",
    "            # Compute Loss of Original Molecule Property\n",
    "            if len(args.aux_task) > 0:\n",
    "                _, _, molvec = models['encoder'](input_X, input_A)\n",
    "                pred_C = models['regressor'](molvec)\n",
    "                list_loss = [reg_loss(pred_C[:, i], input_C[:, i]) for i, label in enumerate(args.aux_task) ]\n",
    "                auxiliary_loss = args.r_lambda * sum(list_loss)\n",
    "                auxiliary_loss.backward()\n",
    "                optimizer['auxiliary'].step(cnt_iter)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            t4 = time.time()\n",
    "            # print(\"total {:2.2f}. Prepare {:2.2f}. Mask {:2.2f}. Aux {:2.2f}\".format(t4-t1, t2-t1, t3-t2, t4-t3))\n",
    "            cnt_iter += 1\n",
    "            setattr(args, 'epoch_now', epoch)\n",
    "            setattr(args, 'iter_now', cnt_iter)\n",
    "\n",
    "            # Prompting Status\n",
    "            if cnt_iter % args.log_every == 0:\n",
    "                train_writer.add_scalar('2.mask_loss/symbol', symbol_loss, cnt_iter)\n",
    "                train_writer.add_scalar('2.mask_loss/degree', degree_loss, cnt_iter)\n",
    "                train_writer.add_scalar('2.mask_loss/numH', numH_loss, cnt_iter)\n",
    "                train_writer.add_scalar('2.mask_loss/valence', valence_loss, cnt_iter)\n",
    "                train_writer.add_scalar('2.mask_loss/isarom', isarom_loss, cnt_iter)\n",
    "                train_writer.add_scalar('1.status/mask', mask_loss, cnt_iter)\n",
    "                if len(args.aux_task) > 0:\n",
    "                    train_writer.add_scalar('1.status/auxiliary', auxiliary_loss, cnt_iter)\n",
    "                    for i, task in enumerate(args.aux_task):\n",
    "                        train_writer.add_scalar('3.auxiliary_loss/{}'.format(task), list_loss[i], cnt_iter)\n",
    "\n",
    "                output = \"[TRAIN] E:{:3}. P:{:>2.1f}%. Loss:{:>9.3}. Mask Loss:{:>9.3}. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "                elapsed = time.time() - t\n",
    "                process_speed = (args.batch_size * args.log_every) / elapsed\n",
    "                output = output.format(epoch, batch_idx / len(dataloader['train']) * 100.0, mask_loss, auxiliary_loss, process_speed, cnt_iter, elapsed,)\n",
    "                t = time.time()\n",
    "                logger.info(output)\n",
    "\n",
    "            # Validate Model\n",
    "            if cnt_iter % args.validate_every == 0:\n",
    "                optimizer['mask'].zero_grad()\n",
    "                optimizer['auxiliary'].zero_grad()\n",
    "                validate(models, dataloader['val'], args, cnt_iter=cnt_iter, epoch=epoch)\n",
    "                t = time.time()\n",
    "\n",
    "            # Save Model\n",
    "            if cnt_iter % args.save_every == 0:\n",
    "                filename = save_checkpoint(epoch, cnt_iter, models, optimizer, args)\n",
    "                logger.info('Saved Model as {}'.format(filename))\n",
    "            del batch\n",
    "                \n",
    "    logger.info('Training Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(models, data_loader, args, **kwargs):\n",
    "    t = time.time()\n",
    "    cnt_iter = kwargs['cnt_iter']\n",
    "    epoch = kwargs['epoch']\n",
    "    temp_iter = 0\n",
    "    reg_loss = nn.MSELoss()\n",
    "\n",
    "    # For Maskingg Task Loss\n",
    "    list_mask_loss = []\n",
    "    list_symbol_loss = []\n",
    "    list_degree_loss = []\n",
    "    list_numH_loss = []\n",
    "    list_valence_loss = []\n",
    "    list_isarom_loss = []\n",
    "\n",
    "    list_symbol_acc = []\n",
    "    list_degree_acc = []\n",
    "    list_numH_acc = []\n",
    "    list_valence_acc = []\n",
    "    list_isarom_acc = []\n",
    "\n",
    "    # For Auxiliary Task Loss\n",
    "    list_aux_loss = []\n",
    "    list_aux_mae = []\n",
    "\n",
    "    # For F1-Score Metric & Confusion Matrix\n",
    "    confusion_symbol = np.zeros((args.vocab_size+1, args.vocab_size+1))\n",
    "    confusion_degree = np.zeros((args.degree_size+1, args.degree_size+1))\n",
    "    confusion_numH = np.zeros((args.numH_size+1, args.numH_size+1))\n",
    "    confusion_valence = np.zeros((args.valence_size+1, args.valence_size+1))\n",
    "    confusion_isarom = np.zeros((args.isarom_size+1, args.isarom_size+1))\n",
    "\n",
    "\n",
    "    # Initialization Model with Evaluation Mode\n",
    "    for _, model in models.items():\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # Get Batch Sample from DataLoader\n",
    "            predict_idx, X, mask_X, true_X, A, C = batch\n",
    "\n",
    "            # Convert Tensor into Variable and Move to CUDA\n",
    "            mask_idx = Variable(predict_idx).to(args.device).long()\n",
    "            input_X = Variable(X).to(args.device).float()\n",
    "            mask_X = Variable(mask_X).to(args.device).float()\n",
    "            true_X = Variable(true_X).to(args.device).float()\n",
    "            input_A = Variable(A).to(args.device).float()\n",
    "            #     mask_A = Variable(mask_A).to(args.device).float()\n",
    "            input_C = Variable(C).to(args.device).float()\n",
    "\n",
    "            # Encoding Masked Molecule\n",
    "            encoded_X, _, molvec = models['encoder'](mask_X, input_A)\n",
    "            pred_X = models['classifier'](encoded_X, molvec, mask_idx)\n",
    "\n",
    "            # Compute Mask Task Loss & Property Regression Loss\n",
    "            symbol_loss, degree_loss, numH_loss, valence_loss, isarom_loss, partial_loss, mask_loss = Compute_loss(pred_X, true_X, args.vocab_size)\n",
    "\n",
    "            list_symbol_loss.append(symbol_loss.item())\n",
    "            list_degree_loss.append(degree_loss.item())\n",
    "            list_numH_loss.append(numH_loss.item())\n",
    "            list_valence_loss.append(valence_loss.item())\n",
    "            list_isarom_loss.append(isarom_loss.item())\n",
    "            list_mask_loss.append((mask_loss).item())\n",
    "\n",
    "            # Compute Mask Task Accuracy & Property Regression MAE\n",
    "            symbol_acc, degree_acc, numH_acc, valence_acc, isarom_acc = compute_metric(pred_X, true_X)\n",
    "            list_symbol_acc.append(symbol_acc)\n",
    "            list_degree_acc.append(degree_acc)\n",
    "            list_numH_acc.append(numH_acc)\n",
    "            list_valence_acc.append(valence_acc)\n",
    "            list_isarom_acc.append(isarom_acc)\n",
    "\n",
    "            # Accumulate Mask Task Confusion Matrix for F1-Metric\n",
    "            confusions = compute_confusion(pred_X, true_X, args)\n",
    "            confusion_symbol += confusions[0]\n",
    "            confusion_degree += confusions[1]\n",
    "            confusion_numH += confusions[2]\n",
    "            confusion_valence += confusions[3]\n",
    "            confusion_isarom += confusions[4]\n",
    "\n",
    "            if len(args.aux_task) > 0:\n",
    "                _, _, molvec = models['encoder'](input_X, input_A)\n",
    "                pred_C = models['regressor'](molvec)\n",
    "                temp_loss = [reg_loss(pred_C[:, i], input_C[:, i]).item() for i, label in enumerate(args.aux_task)]\n",
    "                list_aux_loss.append(temp_loss)\n",
    "\n",
    "                pred_C = pred_C.cpu().detach().numpy()\n",
    "                input_C = input_C.cpu().detach().numpy()\n",
    "                list_aux_mae.append([mean_absolute_error(pred_C[:, i], input_C[:, i]) for i, label in enumerate(args.aux_task)])\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            temp_iter += 1\n",
    "\n",
    "            # Prompting Status\n",
    "            if temp_iter % (args.log_every * 10) == 0:\n",
    "                output = \"[VALID] E:{:3}. P:{:>2.1f}%. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "                elapsed = time.time() - t\n",
    "                process_speed = (args.test_batch_size * args.log_every) / elapsed\n",
    "                output = output.format(epoch, batch_idx / len(data_loader) * 100.0, process_speed, temp_iter, elapsed, )\n",
    "                t = time.time()\n",
    "                logger.info(output)\n",
    "            del batch\n",
    "\n",
    "    val_writer.add_figure('symbol/confusion',\n",
    "                         plot_confusion_matrix(\n",
    "                             confusion_symbol, range(args.vocab_size),\n",
    "                             classes=LIST_SYMBOLS, title=\"Symbol CM @ {}\".format(cnt_iter), figsize=(10, 10)),\n",
    "                         cnt_iter)\n",
    "    val_writer.add_figure('degree/confusion',\n",
    "                         plot_confusion_matrix(confusion_degree[1:, 1:], range(args.degree_size), title=\"Degree CM @ {}\".format(cnt_iter)),\n",
    "                         cnt_iter)\n",
    "    val_writer.add_figure('numH/confusion',\n",
    "                         plot_confusion_matrix(confusion_numH[1:, 1:], range(args.numH_size), title=\"NumH CM @ {}\".format(cnt_iter)),\n",
    "                         cnt_iter)\n",
    "    val_writer.add_figure('valence/confusion',\n",
    "                         plot_confusion_matrix(confusion_valence[1:, 1:], range(args.valence_size), title=\"Valence CM @ {}\".format(cnt_iter)),\n",
    "                         cnt_iter)\n",
    "    val_writer.add_figure('isarom/confusion',\n",
    "                         plot_confusion_matrix(confusion_isarom[1:, 1:], range(args.isarom_size),\n",
    "                                               title=\"isAromatic CM @ {}\".format(cnt_iter), figsize=(2,2)),\n",
    "                         cnt_iter)\n",
    "\n",
    "    # Averaging Loss across the batch\n",
    "    mask_loss = np.mean(np.array(list_mask_loss))\n",
    "    symbol_loss = np.mean(np.array(list_symbol_loss))\n",
    "    degree_loss = np.mean(np.array(list_degree_loss))\n",
    "    numH_loss = np.mean(np.array(list_numH_loss))\n",
    "    valence_loss = np.mean(np.array(list_valence_loss))\n",
    "    isarom_loss = np.mean(np.array(list_isarom_loss))\n",
    "\n",
    "    symbol_acc = np.mean(np.array(list_symbol_acc))\n",
    "    degree_acc = np.mean(np.array(list_degree_acc))\n",
    "    numH_acc = np.mean(np.array(list_numH_acc))\n",
    "    valence_acc = np.mean(np.array(list_valence_acc))\n",
    "    isarom_acc = np.mean(np.array(list_isarom_acc))\n",
    "\n",
    "    val_writer.add_scalar('2.mask_loss/symbol', symbol_loss, cnt_iter)\n",
    "    val_writer.add_scalar('2.mask_loss/degree', degree_loss, cnt_iter)\n",
    "    val_writer.add_scalar('2.mask_loss/numH', numH_loss, cnt_iter)\n",
    "    val_writer.add_scalar('2.mask_loss/valence', valence_loss, cnt_iter)\n",
    "    val_writer.add_scalar('2.mask_loss/isarom', isarom_loss, cnt_iter)\n",
    "\n",
    "    val_writer.add_scalar('4.mask_metric/acc_symbol', symbol_acc, cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/acc_degree', degree_acc, cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/acc_numH', numH_acc, cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/acc_valence', valence_acc, cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/acc_isarom', isarom_acc, cnt_iter)\n",
    "\n",
    "    val_writer.add_scalar('4.mask_metric/f1_symbol', f1_macro(confusion_symbol[1:, 1:]), cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/f1_degree', f1_macro(confusion_degree[1:, 1:]), cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/f1_numH', f1_macro(confusion_numH[1:, 1:]), cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/f1_valence', f1_macro(confusion_valence[1:, 1:]), cnt_iter)\n",
    "    val_writer.add_scalar('4.mask_metric/f1_isarom', f1_macro(confusion_isarom[1:, 1:]), cnt_iter)\n",
    "\n",
    "    if len(args.aux_task) > 0:\n",
    "        list_aux_loss = np.mean(list_aux_loss, axis=0)\n",
    "        list_aux_mae = np.mean(list_aux_mae, axis=0)\n",
    "\n",
    "        for i, task in enumerate(args.aux_task):\n",
    "            val_writer.add_scalar('3.auxiliary_loss/{}'.format(task), list_aux_loss[i], cnt_iter)\n",
    "            val_writer.add_scalar('5.auxiliary_mae/{}'.format(task), list_aux_mae[i], cnt_iter)\n",
    "\n",
    "        auxiliary_loss = np.mean(list_aux_loss)\n",
    "        val_writer.add_scalar('1.status/auxiliary', auxiliary_loss, cnt_iter)\n",
    "    val_writer.add_scalar('1.status/mask', mask_loss, cnt_iter)\n",
    "\n",
    "    # Log model weight historgram\n",
    "    log_histogram(models, val_writer, cnt_iter)\n",
    "\n",
    "    output = \"[VALID] E:{:3}. P:{:>2.1f}%. Mask Loss:{:>9.3}. Aux Loss:{:>9.3}. {:4.1f} mol/sec. Iter:{:6}.  Elapsed:{:6.1f} sec.\"\n",
    "    elapsed = time.time() - t\n",
    "    process_speed = (args.test_batch_size * args.log_every) / elapsed\n",
    "    output = output.format(epoch, batch_idx / len(data_loader) * 100.0, mask_loss, auxiliary_loss, process_speed, cnt_iter, elapsed)\n",
    "    logger.info(output)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "def experiment(dataloader, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    # Construct Model\n",
    "    num_aux_task = len(args.aux_task)\n",
    "    encoder = Encoder(args)\n",
    "    classifier = Classifier(args.out_dim, args.molvec_dim, args.classifier_dim, args.in_dim, args.cdp_rate, ACT2FN[args.act])\n",
    "\n",
    "    models = {'encoder': encoder, 'classifier': classifier}\n",
    "    if len(args.aux_task) > 0:\n",
    "        regressor = Regressor(args.molvec_dim, args.regressor_dim, num_aux_task, args.rdp_rate, ACT2FN[args.act])\n",
    "        models.update({'regressor': regressor})\n",
    "\n",
    "    # Initialize Optimizer\n",
    "    logger.info('####### Model Constructed #######')\n",
    "    mask_trainable_parameters = list()\n",
    "    auxiliary_trainable_parameters = list()\n",
    "    for key, model in models.items():\n",
    "        model.to(args.device)\n",
    "        if key in ['encoder', 'classifier']:\n",
    "            mask_trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        if key in ['encoder', 'regressor']:\n",
    "            auxiliary_trainable_parameters += list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        logger.info('{:10}: {:>10} parameters'.format(key, sum(p.numel() for p in model.parameters())))\n",
    "        setattr(args, '{}_param'.format(key), sum(p.numel() for p in model.parameters()))\n",
    "    logger.info('#################################')\n",
    "    \n",
    "    if args.optim == 'ADAM':\n",
    "        mask_optimizer = optim.Adam(mask_trainable_parameters, lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "        auxiliary_optimizer = optim.Adam(auxiliary_trainable_parameters, lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        mask_optimizer = optim.RMSprop(mask_trainable_parameters, lr=0)\n",
    "        auxiliary_optimizer = optim.RMSprop(auxiliary_trainable_parameters, lr=0)\n",
    "    elif args.optim == 'SGD':\n",
    "        mask_optimizer = optim.SGD(mask_trainable_parameters, lr=0)\n",
    "        auxiliary_optimizer = optim.SGD(auxiliary_trainable_parameters, lr=0)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "    optimizers = {'mask':mask_optimizer, 'auxiliary':auxiliary_optimizer}\n",
    "\n",
    "    # Reload Checkpoint Model\n",
    "    epoch = 0\n",
    "    cnt_iter = 0\n",
    "    if args.ck_filename:\n",
    "        epoch, cnt_iter, models, optimizers = load_checkpoint(models, optimizers, args.ck_filename, args)\n",
    "        logger.info('Loaded Model from {}'.format(args.ck_filename))\n",
    "    \n",
    "    mask_optimizer = NoamOpt(args.out_dim, args.lr_factor, args.lr_step, optimizers['mask'])\n",
    "    auxiliary_optimizer = NoamOpt(args.out_dim, args.lr_factor, args.lr_step, optimizers['auxiliary'])\n",
    "    optimizers = {'mask':mask_optimizer, 'auxiliary':auxiliary_optimizer}\n",
    "\n",
    "    # Train Model\n",
    "    validate(models, dataloader['val'], args, cnt_iter=cnt_iter, epoch=epoch)\n",
    "    train(models, optimizers, dataloader, epoch, cnt_iter, args)\n",
    "\n",
    "    # Logging Experiment Result\n",
    "    te = time.time()    \n",
    "    args.elapsed = te-ts\n",
    "    logger.info('Training Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "##### SIZE #####\n",
    "args.vocab_size = 40\n",
    "args.degree_size = 6\n",
    "args.numH_size = 5\n",
    "args.valence_size = 6\n",
    "args.isarom_size = 2\n",
    "args.in_dim = 64\n",
    "args.out_dim = 256\n",
    "args.molvec_dim = 256\n",
    "args.classifier_dim = 500\n",
    "args.regressor_dim = 500\n",
    "\n",
    "##### MODEL #####\n",
    "args.n_layer = 6\n",
    "args.use_attn = True\n",
    "args.n_attn_heads = 8\n",
    "args.act = 'gelu'\n",
    "args.use_bn = True\n",
    "args.sc_type = 'sc'\n",
    "args.emb_train = True\n",
    "args.train_logp = True\n",
    "args.train_mr = True\n",
    "args.train_tpsa = True\n",
    "args.train_sas = True\n",
    "args.train_mw = True\n",
    "args.aux_task = ['logP', 'mr', 'tpsa', 'sas', 'mw']\n",
    "\n",
    "##### HYPERPARAMETERS #####\n",
    "args.optim = 'ADAM'\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.dp_rate = 0.1\n",
    "args.cdp_rate = 0.3\n",
    "args.rdp_rate = 0.3\n",
    "args.lr_factor = 1.0\n",
    "args.lr_step = 4000\n",
    "args.r_lambda = 1.0\n",
    "\n",
    "##### EXP #####\n",
    "args.epoch = 100\n",
    "args.batch_size = 512\n",
    "args.test_batch_size = 512\n",
    "args.save_every = 100\n",
    "args.validate_every = 100\n",
    "args.log_every = 20\n",
    "\n",
    "\n",
    "##### DEVICE #####\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "##### LOGGING #####\n",
    "args.log_path = 'runs'\n",
    "args.model_name = 'exp_test3'\n",
    "args.model_explain = make_model_comment(args)\n",
    "train_writer = SummaryWriter(join(args.log_path, args.model_name+'_train'))\n",
    "val_writer = SummaryWriter(join(args.log_path, args.model_name+'_val'))\n",
    "train_writer.add_text(tag='model', text_string='{}:{}'.format(args.model_name, args.model_explain), global_step= 0)\n",
    "logger = get_logger(join(args.log_path, args.model_name+'_train'))\n",
    "\n",
    "##### RESUME TRAINING #####\n",
    "args.ck_filename = None # Example: 'model_ck_000_000000100.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/xxs/train'\n",
    "val_dataset_path = './dataset/xxs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)\n",
    "list_vals = get_dir_files(val_dataset_path)\n",
    "\n",
    "train_dataset = zincDataset(train_dataset_path, list_trains[0], 8)\n",
    "sampler = SequentialSampler(train_dataset)\n",
    "SortedBatchSampler = BatchSampler(sampler=sampler, batch_size=args.batch_size, drop_last=True, shuffle_batch=True)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  num_workers=8,\n",
    "                                  collate_fn=postprocess_batch,\n",
    "                                  batch_sampler=SortedBatchSampler)\n",
    "\n",
    "val_dataset = zincDataset(val_dataset_path, list_vals[0], 8)\n",
    "sampler = SequentialSampler(val_dataset)\n",
    "SortedBatchSampler = BatchSampler(sampler=sampler, batch_size=args.test_batch_size, drop_last=True, shuffle_batch=False)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                                num_workers=8,\n",
    "                                collate_fn=postprocess_batch,\n",
    "                                batch_sampler=SortedBatchSampler)\n",
    "\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:11:42,018 [INFO] ####### Model Constructed #######\n",
      "2018-12-04 02:11:42,018 [INFO] ####### Model Constructed #######\n",
      "2018-12-04 02:11:42,028 [INFO] encoder   :     547985 parameters\n",
      "2018-12-04 02:11:42,028 [INFO] encoder   :     547985 parameters\n",
      "2018-12-04 02:11:42,030 [INFO] classifier:      48911 parameters\n",
      "2018-12-04 02:11:42,030 [INFO] classifier:      48911 parameters\n",
      "2018-12-04 02:11:42,032 [INFO] logP      :     131585 parameters\n",
      "2018-12-04 02:11:42,032 [INFO] logP      :     131585 parameters\n",
      "2018-12-04 02:11:42,034 [INFO] mr        :     131585 parameters\n",
      "2018-12-04 02:11:42,034 [INFO] mr        :     131585 parameters\n",
      "2018-12-04 02:11:42,035 [INFO] tpsa      :     131585 parameters\n",
      "2018-12-04 02:11:42,035 [INFO] tpsa      :     131585 parameters\n",
      "2018-12-04 02:11:42,036 [INFO] #################################\n",
      "2018-12-04 02:11:42,036 [INFO] #################################\n",
      "2018-12-04 02:11:48,356 [INFO] [T] E:  0. P:2.3%. Loss:     10.7. Mask Loss:     10.7. 1620.8 mol/sec. Iter:    20.  Elapsed:   6.3 sec.\n",
      "2018-12-04 02:11:48,356 [INFO] [T] E:  0. P:2.3%. Loss:     10.7. Mask Loss:     10.7. 1620.8 mol/sec. Iter:    20.  Elapsed:   6.3 sec.\n",
      "2018-12-04 02:11:53,104 [INFO] [T] E:  0. P:4.7%. Loss:     7.76. Mask Loss:     7.76. 2156.5 mol/sec. Iter:    40.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:11:53,104 [INFO] [T] E:  0. P:4.7%. Loss:     7.76. Mask Loss:     7.76. 2156.5 mol/sec. Iter:    40.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:11:57,855 [INFO] [T] E:  0. P:7.1%. Loss:     7.63. Mask Loss:     7.63. 2155.5 mol/sec. Iter:    60.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:11:57,855 [INFO] [T] E:  0. P:7.1%. Loss:     7.63. Mask Loss:     7.63. 2155.5 mol/sec. Iter:    60.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:02,694 [INFO] [T] E:  0. P:9.5%. Loss:     8.68. Mask Loss:     8.68. 2116.0 mol/sec. Iter:    80.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:02,694 [INFO] [T] E:  0. P:9.5%. Loss:     8.68. Mask Loss:     8.68. 2116.0 mol/sec. Iter:    80.  Elapsed:   4.8 sec.\n",
      "2018-12-04 02:12:07,390 [INFO] Saved Model as model_ck_000_000000100.tar\n",
      "2018-12-04 02:12:07,390 [INFO] Saved Model as model_ck_000_000000100.tar\n",
      "2018-12-04 02:12:15,233 [INFO] [V] E:  0. P:37.6%. 1306.5 mol/sec. Iter:    80.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:15,233 [INFO] [V] E:  0. P:37.6%. 1306.5 mol/sec. Iter:    80.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:23,041 [INFO] [V] E:  0. P:75.7%. 1311.4 mol/sec. Iter:   160.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:23,041 [INFO] [V] E:  0. P:75.7%. 1311.4 mol/sec. Iter:   160.  Elapsed:   7.8 sec.\n",
      "2018-12-04 02:12:28,480 [INFO] [V] E:  0. P:99.5%. Loss:     7.45. Mask Loss:     5.22. 1883.0 mol/sec. Iter:   100.  Elapsed:   5.4 sec.\n",
      "2018-12-04 02:12:28,480 [INFO] [V] E:  0. P:99.5%. Loss:     7.45. Mask Loss:     5.22. 1883.0 mol/sec. Iter:   100.  Elapsed:   5.4 sec.\n",
      "2018-12-04 02:12:28,663 [INFO] [T] E:  0. P:11.9%. Loss:     6.75. Mask Loss:     6.75. 1533916891.4 mol/sec. Iter:   100.  Elapsed:   0.0 sec.\n",
      "2018-12-04 02:12:28,663 [INFO] [T] E:  0. P:11.9%. Loss:     6.75. Mask Loss:     6.75. 1533916891.4 mol/sec. Iter:   100.  Elapsed:   0.0 sec.\n",
      "2018-12-04 02:12:33,602 [INFO] [T] E:  0. P:14.4%. Loss:     6.58. Mask Loss:     6.58. 2073.2 mol/sec. Iter:   120.  Elapsed:   4.9 sec.\n",
      "2018-12-04 02:12:33,602 [INFO] [T] E:  0. P:14.4%. Loss:     6.58. Mask Loss:     6.58. 2073.2 mol/sec. Iter:   120.  Elapsed:   4.9 sec.\n",
      "2018-12-04 02:12:38,583 [INFO] [T] E:  0. P:16.8%. Loss:     7.35. Mask Loss:     7.35. 2055.7 mol/sec. Iter:   140.  Elapsed:   5.0 sec.\n",
      "2018-12-04 02:12:38,583 [INFO] [T] E:  0. P:16.8%. Loss:     7.35. Mask Loss:     7.35. 2055.7 mol/sec. Iter:   140.  Elapsed:   5.0 sec.\n",
      "2018-12-04 02:12:43,326 [INFO] [T] E:  0. P:19.2%. Loss:     7.13. Mask Loss:     7.13. 2159.3 mol/sec. Iter:   160.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:43,326 [INFO] [T] E:  0. P:19.2%. Loss:     7.13. Mask Loss:     7.13. 2159.3 mol/sec. Iter:   160.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:48,050 [INFO] [T] E:  0. P:21.6%. Loss:     6.94. Mask Loss:     6.94. 2167.7 mol/sec. Iter:   180.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:48,050 [INFO] [T] E:  0. P:21.6%. Loss:     6.94. Mask Loss:     6.94. 2167.7 mol/sec. Iter:   180.  Elapsed:   4.7 sec.\n",
      "2018-12-04 02:12:52,927 [INFO] Saved Model as model_ck_000_000000200.tar\n",
      "2018-12-04 02:12:52,927 [INFO] Saved Model as model_ck_000_000000200.tar\n",
      "2018-12-04 02:13:00,414 [INFO] [V] E:  0. P:37.6%. 1368.4 mol/sec. Iter:    80.  Elapsed:   7.5 sec.\n",
      "2018-12-04 02:13:00,414 [INFO] [V] E:  0. P:37.6%. 1368.4 mol/sec. Iter:    80.  Elapsed:   7.5 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-545d40d4a4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-864c9820963d>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dataloader, args)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Logging Experiment Result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-864c9820963d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, optimizer, dataloader, epoch, cnt_iter, args)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnt_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-093ef6fd4565>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(models, data_loader, args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0minput_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0minput_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comet/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "logger.info(\"######## Starting Training ########\")\n",
    "result = experiment(dataloader, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comet)",
   "language": "python",
   "name": "comet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
