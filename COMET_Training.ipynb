{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "from torchtext.data import RawField, Field, TabularDataset, BucketIterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler, SequentialSampler\n",
    "from torch._six import int_classes as _int_classes\n",
    "\n",
    "from rdkit import Chem\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MASKING_RATE = 0.15\n",
    "ERASE_RATE = 0.5\n",
    "\n",
    "def get_dir_files(dir_path):\n",
    "    list_file = [f for f in os.listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    return list_file\n",
    "\n",
    "train_dataset_path = './dataset/processed_zinc_smiles/data_xs/train'\n",
    "val_dataset_path = './dataset/processed_zinc_smiles/data_xs/val'\n",
    "\n",
    "list_trains = get_dir_files(train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_feature(atom):\n",
    "    return np.array(char_to_ix(atom.GetSymbol(),\n",
    "                              ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                               'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                               'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                               'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding_unk(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def char_to_ix(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        return [0] # Unknown Atom Token\n",
    "    return [allowable_set.index(x)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class zincDataset(Dataset):\n",
    "    def __init__(self, data_path, skip_header=True):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.data = self.data.sort_values(by=['length'])\n",
    "        self.data = self.data.reset_index()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.loc[index, :]\n",
    "        smile = row.smile\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
    "        list_feature = list()\n",
    "        for atom in mol.GetAtoms():\n",
    "            list_feature.append(atom_feature(atom))\n",
    "        \n",
    "        return row.length, np.array(list_feature), adj, row.logP, row.mr, row.tpsa\n",
    "    \n",
    "    def get_sizes(self):\n",
    "        return self.data['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_onehot(size):\n",
    "    \"\"\" Generate random one-hot encoding vector with given size. \"\"\"\n",
    "    temp = np.zeros(size)\n",
    "    temp[np.random.randint(0, size)] = 1\n",
    "    return temp \n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\" Symmetry Normalization \"\"\"\n",
    "    rowsum = np.diag(np.array(mx.sum(1)))\n",
    "    r_inv = fractional_matrix_power(rowsum, -0.5)\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    return r_inv.dot(mx).dot(r_inv)\n",
    "\n",
    "def masking_feature(feature, num_masking):\n",
    "    \"\"\" Given feature, select 'num_masking' node feature and perturbate them.\n",
    "    \n",
    "        [5 features : Atom symbol, degree, num Hs, valence, isAromatic]  \n",
    "        were masked with zero or changed with random one-hot encoding \n",
    "        or remained with origianl data(but still should be predicted).\n",
    "        \n",
    "        Masking process was conducted on each feature indiviually. \n",
    "        For example, if ERASE_RATE = 0.5, probability for all feature information with zero is 0.5^5 = 0.03125\n",
    "        \n",
    "        return original hode feature with their corresponding indices\n",
    "    \"\"\"\n",
    "    \n",
    "    masking_indices = np.random.choice(len(feature), num_masking, replace=False)\n",
    "    ground_truth = np.copy(feature[masking_indices, :])\n",
    "    for i in masking_indices:\n",
    "        prob_masking = np.random.rand(5)\n",
    "        # Masking Atom Symbol \n",
    "        if prob_masking[0] < ERASE_RATE:\n",
    "            feature[i, 0] = 0\n",
    "        elif prob_masking[0] > 1- ((1-ERASE_RATE) * 0.5):\n",
    "            feature[i, 0] = np.random.randint(1, 41)\n",
    "            \n",
    "        # Masking Degree \n",
    "        if prob_masking[1] < ERASE_RATE:\n",
    "            feature[i, 1:7] = np.zeros(6)\n",
    "        elif prob_masking[1] > 1- ((1-ERASE_RATE) * 0.5):\n",
    "            feature[i, 1:7] =  random_onehot(6)\n",
    "        \n",
    "        # Masking Num Hs\n",
    "        if prob_masking[2] < ERASE_RATE:\n",
    "            feature[i, 7:12] = np.zeros(5)\n",
    "        elif prob_masking[2] > 1- ((1-ERASE_RATE) * 0.5):\n",
    "            feature[i, 7:12] =  random_onehot(5)\n",
    "            \n",
    "        # Masking Valence\n",
    "        if prob_masking[3] < ERASE_RATE:\n",
    "            feature[i, 12:18] = np.zeros(6)\n",
    "        elif prob_masking[3] > 1- ((1-ERASE_RATE) * 0.5):\n",
    "            feature[i, 12:18] =  random_onehot(6)\n",
    "            \n",
    "        # Masking IsAromatic\n",
    "        if prob_masking[4] < ERASE_RATE:\n",
    "            feature[i, 18] = (feature[i, 18]+1)%2\n",
    "\n",
    "    return feature, ground_truth, masking_indices\n",
    "\n",
    "\n",
    "def postprocess_batch(mini_batch):\n",
    "    \"\"\" Given mini-batch sample, adjacency matrix and node feature vectors were padded with zero. \"\"\"\n",
    "    max_length = max([row[0] for row in mini_batch])\n",
    "    min_length = min([row[0] for row in mini_batch])\n",
    "    print(min_length, max_length)\n",
    "    num_masking = int(max_length * MASKING_RATE)\n",
    "    batch_length = len(mini_batch)\n",
    "    batch_feature = np.zeros((batch_length, max_length, mini_batch[0][1].shape[1]), dtype=int)\n",
    "    batch_adj = np.zeros((batch_length, max_length, max_length))\n",
    "    batch_property = np.zeros((batch_length, 3))\n",
    "    batch_ground = np.zeros((batch_length, num_masking, mini_batch[0][1].shape[1]), dtype=int)\n",
    "    batch_masking = np.zeros((batch_length, num_masking), dtype=int)\n",
    "    \n",
    "    for i, row in enumerate(mini_batch):\n",
    "        mol_length, feature, adj = row[0], row[1], row[2]\n",
    "        masked_feature, ground_truth, masking_indices  = masking_feature(feature, num_masking)\n",
    "        batch_feature[i, :mol_length, :] = masked_feature\n",
    "        batch_ground[i, :, :] = ground_truth\n",
    "        batch_masking[i, :] = masking_indices\n",
    "        batch_adj[i, :mol_length, :mol_length] = normalize_adj(adj+np.eye(len(adj)))\n",
    "        batch_property[i, :] = [row[3], row[4], row[5]]\n",
    "        \n",
    "    return batch_feature, batch_adj, batch_property, batch_ground, batch_masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(Sampler):\n",
    "\n",
    "    def __init__(self, sampler, batch_size, drop_last=False, shuffle_batch=False):\n",
    "\n",
    "        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n",
    "                batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integeral value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(\"drop_last should be a boolean value, but got \"\n",
    "                             \"drop_last={}\".format(drop_last))\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle_batch = shuffle_batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = list()\n",
    "        mini_batch = list()\n",
    "        for idx in self.sampler:\n",
    "            mini_batch.append(idx)\n",
    "            if len(mini_batch) == self.batch_size:\n",
    "                batch.append(mini_batch)\n",
    "                mini_batch = []\n",
    "        if len(mini_batch) > 0 and not self.drop_last:\n",
    "            batch.append(mini_batch)\n",
    "            \n",
    "        if self.shuffle_batch:\n",
    "            return iter(np.random.permutation(batch))\n",
    "        else:\n",
    "            return iter(batch)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = zincDataset(data_path=join(train_dataset_path, list_trains[0])) './train000000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n",
      "14 14\n",
      "14 14\n",
      "13 14\n",
      "6 11\n",
      "12 12\n",
      "11 12\n",
      "13 13\n",
      "12 12\n",
      "13 13\n",
      "13 13\n",
      "12 13\n",
      "14 14\n",
      "14 14\n",
      "15 15\n",
      "15 15\n",
      "14 15\n",
      "14 14\n",
      "14 14\n",
      "14 14\n",
      "14 14\n",
      "14 14\n",
      "14 14\n",
      "14 14\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 16\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "16 16\n",
      "16 16\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "sampler = SequentialSampler(train_dataset)\n",
    "\n",
    "SortedBatchSampler = BatchSampler(sampler=sampler,\n",
    "                                  batch_size=100,\n",
    "                                  drop_last=False,\n",
    "                                  shuffle_batch=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              collate_fn=postprocess_batch, \n",
    "                              num_workers=12, \n",
    "                              batch_sampler=SortedBatchSampler)\n",
    "# print(next(train_dataloader.__iter__())[3])\n",
    "cnt = 0\n",
    "for batch in train_dataloader:\n",
    "#     print(batch[0])\n",
    "    cnt += 1\n",
    "    if cnt > 30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
